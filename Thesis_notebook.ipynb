{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e3efb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on cuda\n",
      "Number of GPUs available: 1\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#from torch import cuda\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "#device\n",
    "print(f\"working on {device}\")\n",
    "# Check the number of available GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {num_gpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaf04eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    RobertaTokenizer,\n",
    "    RobertaModel,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "129e3cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    '../data/anger_frustration.csv',\n",
    "    '../data/confusion_sorrow_boredom.csv',\n",
    "    '../data/disgust_surprise_alarm_fear.csv',\n",
    "    '../data/interest_desire.csv',\n",
    "    '../data/joy_hope.csv',\n",
    "    '../data/understanding_gratitude_relief.csv'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a3337ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Behavior ID</th>\n",
       "      <th>Description</th>\n",
       "      <th>All</th>\n",
       "      <th>NumIndexes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anger_frustration</td>\n",
       "      <td>anger000</td>\n",
       "      <td>Say \"No!\" &gt; Display Face2 &gt; Drive forward quic...</td>\n",
       "      <td>('say_text', 'No!', 1, 0)\\r\\n('display_oled_fa...</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anger_frustration</td>\n",
       "      <td>anger001</td>\n",
       "      <td>Say \"Oon!\" &gt; Turn 30deg quickly &gt; Raise arms h...</td>\n",
       "      <td>('say_text', 'Oon!', 1, 0)\\r\\n('turn_in_place'...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anger_frustration</td>\n",
       "      <td>anger002</td>\n",
       "      <td>Display Face2 &gt; Say \"Gurr!\" &gt; Lift arms quickl...</td>\n",
       "      <td>('display_oled_face_image', 'resources/cozmo_f...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anger_frustration</td>\n",
       "      <td>anger003</td>\n",
       "      <td>Raise head &gt; Lift arms quickly &gt; Turn 30deg qu...</td>\n",
       "      <td>('set_head_angle', 45, 1)\\r\\n('set_lift_height...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anger_frustration</td>\n",
       "      <td>anger004</td>\n",
       "      <td>Raise arms halfway &gt; Display Face2 &gt; Drive lon...</td>\n",
       "      <td>('set_lift_height', 0.5, 0.2)\\r\\n('display_ole...</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>understanding_gratitude_relief</td>\n",
       "      <td>understanding177</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>understanding_gratitude_relief</td>\n",
       "      <td>understanding178</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>understanding_gratitude_relief</td>\n",
       "      <td>understanding179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>understanding_gratitude_relief</td>\n",
       "      <td>understanding180</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>understanding_gratitude_relief</td>\n",
       "      <td>understanding181</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1040 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Emotions       Behavior ID  \\\n",
       "0                  anger_frustration          anger000   \n",
       "1                  anger_frustration          anger001   \n",
       "2                  anger_frustration          anger002   \n",
       "3                  anger_frustration          anger003   \n",
       "4                  anger_frustration          anger004   \n",
       "...                              ...               ...   \n",
       "1035  understanding_gratitude_relief  understanding177   \n",
       "1036  understanding_gratitude_relief  understanding178   \n",
       "1037  understanding_gratitude_relief  understanding179   \n",
       "1038  understanding_gratitude_relief  understanding180   \n",
       "1039  understanding_gratitude_relief  understanding181   \n",
       "\n",
       "                                            Description  \\\n",
       "0     Say \"No!\" > Display Face2 > Drive forward quic...   \n",
       "1     Say \"Oon!\" > Turn 30deg quickly > Raise arms h...   \n",
       "2     Display Face2 > Say \"Gurr!\" > Lift arms quickl...   \n",
       "3     Raise head > Lift arms quickly > Turn 30deg qu...   \n",
       "4     Raise arms halfway > Display Face2 > Drive lon...   \n",
       "...                                                 ...   \n",
       "1035                                                NaN   \n",
       "1036                                                NaN   \n",
       "1037                                                NaN   \n",
       "1038                                                NaN   \n",
       "1039                                                NaN   \n",
       "\n",
       "                                                    All  NumIndexes  \n",
       "0     ('say_text', 'No!', 1, 0)\\r\\n('display_oled_fa...         6.0  \n",
       "1     ('say_text', 'Oon!', 1, 0)\\r\\n('turn_in_place'...         5.0  \n",
       "2     ('display_oled_face_image', 'resources/cozmo_f...         9.0  \n",
       "3     ('set_head_angle', 45, 1)\\r\\n('set_lift_height...         7.0  \n",
       "4     ('set_lift_height', 0.5, 0.2)\\r\\n('display_ole...         6.0  \n",
       "...                                                 ...         ...  \n",
       "1035                                                NaN         NaN  \n",
       "1036                                                NaN         NaN  \n",
       "1037                                                NaN         NaN  \n",
       "1038                                                NaN         NaN  \n",
       "1039                                                NaN         NaN  \n",
       "\n",
       "[1040 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "general_labels = {\n",
    "    'anger_frustration.csv': 'anger_frustration',\n",
    "    'confusion_sorrow_boredom.csv': 'confusion_sorrow_boredom',\n",
    "    'disgust_surprise_alarm_fear.csv': 'disgust_surprise_alarm_fear',\n",
    "    'interest_desire.csv': 'interest_desire',\n",
    "    'joy_hope.csv': 'joy_hope',\n",
    "    'understanding_gratitude_relief.csv': 'understanding_gratitude_relief'\n",
    "}\n",
    "\n",
    "# Function to extract the general label from the file path\n",
    "def extract_general_label(file_path):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    return general_labels.get(file_name, 'unknown')\n",
    "\n",
    "# Load CSV files into DataFrames and add general labels\n",
    "dataframes = []\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_csv(file_path)\n",
    "    general_label = extract_general_label(file_path)\n",
    "    df['Emotions'] = general_label  # Set the general label for all rows\n",
    "    dataframes.append(df)\n",
    "\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the combined DataFrame\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28cb7cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handles missing value for the \"All\" column\n",
    "data.dropna(subset=[\"All\"], inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02f0e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_text(actions_str):\n",
    "    # Regex pattern to match the tuples in your string\n",
    "    pattern = r\"\\(([^)]+)\\)\"\n",
    "    \n",
    "    # Find all tuples in the string\n",
    "    matches = re.findall(pattern, actions_str)\n",
    "    \n",
    "    result = []\n",
    "    for match in matches:\n",
    "        # Split each tuple's content by commas, remove extra spaces\n",
    "        elements = [elem.strip() for elem in match.split(\",\")]\n",
    "        \n",
    "        # Rebuild the action string, converting numbers and removing quotes\n",
    "        action = elements[0].strip(\"'\")  # Get the action name\n",
    "        args = [arg.strip(\"'\") if \"'\" in arg else arg for arg in elements[1:]]  # Process arguments\n",
    "        action_str = \"_\".join([action] + args)\n",
    "        \n",
    "        # Special case: Remove file path in display_oled_face_image\n",
    "        if action == 'display_oled_face_image':\n",
    "            face_identifier = args[0].split('/')[-1].replace('.png', '')\n",
    "            action_str = f\"display_oled_face_image_{face_identifier}_{args[1]}\"\n",
    "        \n",
    "        # Remove all special characters\n",
    "        #action_str = re.sub(r'[^a-zA-Z0-9_]', '', action_str)\n",
    "        \n",
    "        result.append(action_str)\n",
    "    \n",
    "    # Join all the action strings with a space\n",
    "    return \" \".join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a797a895",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Behaviors'] = data['All'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14ac0a8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion label distribution:\n",
      " Emotions\n",
      "disgust_surprise_alarm_fear       159\n",
      "confusion_sorrow_boredom          123\n",
      "anger_frustration                  84\n",
      "joy_hope                           84\n",
      "interest_desire                    78\n",
      "understanding_gratitude_relief     68\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the occurrences of each emotion label\n",
    "emotion_counts = data['Emotions'].value_counts()\n",
    "print(\"Emotion label distribution:\\n\", emotion_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5dac475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Behavior ID</th>\n",
       "      <th>Description</th>\n",
       "      <th>All</th>\n",
       "      <th>NumIndexes</th>\n",
       "      <th>Behaviors</th>\n",
       "      <th>Emotion_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anger_frustration</td>\n",
       "      <td>anger000</td>\n",
       "      <td>Say \"No!\" &gt; Display Face2 &gt; Drive forward quic...</td>\n",
       "      <td>('say_text', 'No!', 1, 0)\\r\\n('display_oled_fa...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>say_text_No!_1_0 display_oled_face_image_face2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anger_frustration</td>\n",
       "      <td>anger001</td>\n",
       "      <td>Say \"Oon!\" &gt; Turn 30deg quickly &gt; Raise arms h...</td>\n",
       "      <td>('say_text', 'Oon!', 1, 0)\\r\\n('turn_in_place'...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>say_text_Oon!_1_0 turn_in_place_30_230 set_lif...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anger_frustration</td>\n",
       "      <td>anger002</td>\n",
       "      <td>Display Face2 &gt; Say \"Gurr!\" &gt; Lift arms quickl...</td>\n",
       "      <td>('display_oled_face_image', 'resources/cozmo_f...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>display_oled_face_image_face2_1 say_text_Gurr!...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anger_frustration</td>\n",
       "      <td>anger003</td>\n",
       "      <td>Raise head &gt; Lift arms quickly &gt; Turn 30deg qu...</td>\n",
       "      <td>('set_head_angle', 45, 1)\\r\\n('set_lift_height...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>set_head_angle_45_1 set_lift_height_1_0.2 turn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anger_frustration</td>\n",
       "      <td>anger004</td>\n",
       "      <td>Raise arms halfway &gt; Display Face2 &gt; Drive lon...</td>\n",
       "      <td>('set_lift_height', 0.5, 0.2)\\r\\n('display_ole...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>set_lift_height_0.5_0.2 display_oled_face_imag...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>understanding_gratitude_relief</td>\n",
       "      <td>understanding051</td>\n",
       "      <td>Turn 30deg &gt; Display Face8 &gt; Lower head &gt; Lift...</td>\n",
       "      <td>('turn_in_place', 30, 100)\\r\\n('display_oled_f...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>turn_in_place_30_100 display_oled_face_image_f...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>understanding_gratitude_relief</td>\n",
       "      <td>understanding052</td>\n",
       "      <td>Turn -30deg &gt; Display Face4 &gt; Lower head &gt; Lif...</td>\n",
       "      <td>('turn_in_place', -30, 100)\\r\\n('display_oled_...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>turn_in_place_-30_100 display_oled_face_image_...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>understanding_gratitude_relief</td>\n",
       "      <td>understanding054</td>\n",
       "      <td>Display Face8 &gt; Display Face9 &gt; Lower head &gt; D...</td>\n",
       "      <td>('display_oled_face_image', 'resources/cozmo_f...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>display_oled_face_image_face8_1 display_oled_f...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>understanding_gratitude_relief</td>\n",
       "      <td>understanding055</td>\n",
       "      <td>Lift arms halfway quickly &gt; Display Face7 &gt; Sa...</td>\n",
       "      <td>('set_lift_height', 0.5, 0.2)\\r\\n('display_ole...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>set_lift_height_0.5_0.2 display_oled_face_imag...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>understanding_gratitude_relief</td>\n",
       "      <td>understanding056</td>\n",
       "      <td>Lift arms halfway quickly &gt; Display Face12 &gt; S...</td>\n",
       "      <td>('set_lift_height', 0.5, 0.2)\\r\\n('display_ole...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>set_lift_height_0.5_0.2 display_oled_face_imag...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>596 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Emotions       Behavior ID  \\\n",
       "0                 anger_frustration          anger000   \n",
       "1                 anger_frustration          anger001   \n",
       "2                 anger_frustration          anger002   \n",
       "3                 anger_frustration          anger003   \n",
       "4                 anger_frustration          anger004   \n",
       "..                              ...               ...   \n",
       "591  understanding_gratitude_relief  understanding051   \n",
       "592  understanding_gratitude_relief  understanding052   \n",
       "593  understanding_gratitude_relief  understanding054   \n",
       "594  understanding_gratitude_relief  understanding055   \n",
       "595  understanding_gratitude_relief  understanding056   \n",
       "\n",
       "                                           Description  \\\n",
       "0    Say \"No!\" > Display Face2 > Drive forward quic...   \n",
       "1    Say \"Oon!\" > Turn 30deg quickly > Raise arms h...   \n",
       "2    Display Face2 > Say \"Gurr!\" > Lift arms quickl...   \n",
       "3    Raise head > Lift arms quickly > Turn 30deg qu...   \n",
       "4    Raise arms halfway > Display Face2 > Drive lon...   \n",
       "..                                                 ...   \n",
       "591  Turn 30deg > Display Face8 > Lower head > Lift...   \n",
       "592  Turn -30deg > Display Face4 > Lower head > Lif...   \n",
       "593  Display Face8 > Display Face9 > Lower head > D...   \n",
       "594  Lift arms halfway quickly > Display Face7 > Sa...   \n",
       "595  Lift arms halfway quickly > Display Face12 > S...   \n",
       "\n",
       "                                                   All  NumIndexes  \\\n",
       "0    ('say_text', 'No!', 1, 0)\\r\\n('display_oled_fa...         6.0   \n",
       "1    ('say_text', 'Oon!', 1, 0)\\r\\n('turn_in_place'...         5.0   \n",
       "2    ('display_oled_face_image', 'resources/cozmo_f...         9.0   \n",
       "3    ('set_head_angle', 45, 1)\\r\\n('set_lift_height...         7.0   \n",
       "4    ('set_lift_height', 0.5, 0.2)\\r\\n('display_ole...         6.0   \n",
       "..                                                 ...         ...   \n",
       "591  ('turn_in_place', 30, 100)\\r\\n('display_oled_f...         9.0   \n",
       "592  ('turn_in_place', -30, 100)\\r\\n('display_oled_...         8.0   \n",
       "593  ('display_oled_face_image', 'resources/cozmo_f...        10.0   \n",
       "594  ('set_lift_height', 0.5, 0.2)\\r\\n('display_ole...         8.0   \n",
       "595  ('set_lift_height', 0.5, 0.2)\\r\\n('display_ole...         8.0   \n",
       "\n",
       "                                             Behaviors  Emotion_Label  \n",
       "0    say_text_No!_1_0 display_oled_face_image_face2...              0  \n",
       "1    say_text_Oon!_1_0 turn_in_place_30_230 set_lif...              0  \n",
       "2    display_oled_face_image_face2_1 say_text_Gurr!...              0  \n",
       "3    set_head_angle_45_1 set_lift_height_1_0.2 turn...              0  \n",
       "4    set_lift_height_0.5_0.2 display_oled_face_imag...              0  \n",
       "..                                                 ...            ...  \n",
       "591  turn_in_place_30_100 display_oled_face_image_f...              5  \n",
       "592  turn_in_place_-30_100 display_oled_face_image_...              5  \n",
       "593  display_oled_face_image_face8_1 display_oled_f...              5  \n",
       "594  set_lift_height_0.5_0.2 display_oled_face_imag...              5  \n",
       "595  set_lift_height_0.5_0.2 display_oled_face_imag...              5  \n",
       "\n",
       "[596 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "data['Emotion_Label'] = label_encoder.fit_transform(data['Emotions'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "813ede05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' is your DataFrame\n",
    "data.to_csv('research_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e87748c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of emotions labels : 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_class=len(data['Emotions'].unique())\n",
    "\n",
    "print(\"Total number of emotions labels :\",num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abd7e32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Prepare the text and labels\n",
    "texts = data['Behaviors'].tolist()\n",
    "# Encode labels using LabelEncoder\n",
    "labels = data['Emotions'].tolist()\n",
    "X_train, X_test_dev, y_train, y_test_dev = train_test_split(texts, labels, stratify = labels, test_size=0.2, random_state=42)\n",
    "X_test, X_dev, y_test, y_dev = train_test_split(X_test_dev, y_test_dev,stratify = y_test_dev, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "852d105c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['display_oled_face_image_face7_1 display_oled_face_image_face6_1 say_text_oh_1_0 display_oled_face_image_face6_1 set_head_angle_-30_1 set_lift_height_0.75_0.2 display_oled_face_image_face6_1 say_text_boo hoo!_1_1',\n",
       " 'turn_in_place_-30_500 display_oled_face_image_face8_1 display_oled_face_image_face9_0.2 display_oled_face_image_face8_0.5 say_text_ah!_1_1 turn_in_place_30_500 display_oled_face_image_face12_1',\n",
       " 'turn_in_place_30_500 display_oled_face_image_face15_1 say_text_gah!_1_0 turn_in_place_-30_500 display_oled_face_image_face15_1 say_text_gurr!_1_1 display_oled_face_image_face2_1 turn_in_place_30_500 display_oled_face_image_face15_1',\n",
       " 'drive_straight_200_50 display_oled_face_image_face6_1 say_text_boo hoo_1_0 set_head_angle_-30_1 display_oled_face_image_face6_1',\n",
       " 'set_head_angle_45_1 display_oled_face_image_face8_1 say_text_Uh huh_1_1 set_lift_height_1_0.2 set_lift_height_0_1 say_text_Okie!_1_1 display_oled_face_image_face8_1',\n",
       " 'say_text_Huhh?_1_1 set_head_angle_20_1 display_oled_face_image_face5_1 say_text_Hmm?_1_1 display_oled_face_image_face4_1',\n",
       " 'turn_in_place_360_500 display_oled_face_image_face7_1 say_text_oh_1_0 turn_in_place_-360_500 display_oled_face_image_face7_1 say_text_oh!_1_1 display_oled_face_image_face9_1 say_text_few_1_0',\n",
       " 'say_text_woot!_1_1 display_oled_face_image_face17_0.2 display_oled_face_image_face18_0.2 turn_in_place_30_500 turn_in_place_-30_500 set_lift_height_0.5_0.2 display_oled_face_image_face8_1 say_text_oh! oh!_1_1',\n",
       " 'display_oled_face_image_face10_1 say_text_uh_1_0 set_head_angle_-30_1 display_oled_face_image_face14_1 set_head_angle_0_1 display_oled_face_image_face14_1 say_text_hmm_1_0',\n",
       " 'set_head_angle_45_0.2 display_oled_face_image_face7_1 say_text_ack!_1_1 set_lift_height_1_0.2 set_lift_height_0_0.2 display_oled_face_image_face7_1 set_head_angle_-30_1 set_lift_height_0.5_1',\n",
       " 'turn_in_place_30_25 display_oled_face_image_face7_1 say_text_um?_1_1 display_oled_face_image_face16_0.5 display_oled_face_image_face7_1 say_text_hmm?_1_1',\n",
       " 'say_text_Uhhhg_1_0 display_oled_face_image_face2_1 set_head_angle_-10_0.2 set_lift_height_0.5_0.2 set_lift_height_0_0.2 set_lift_height_0.5_0.2 set_lift_height_0_0.2 display_oled_face_image_face5_1',\n",
       " 'display_oled_face_image_face8_1 set_head_angle_30_0.2 turn_in_place_360_500 say_text_woo!_1_1 drive_straight_50_500 say_text_yay!_1_1 display_oled_face_image_face8_1',\n",
       " 'turn_in_place_60_50 display_oled_face_image_face14_1 say_text_hmm_1_0 turn_in_place_-120_50 display_oled_face_image_face7_1 say_text_ah_1_1 turn_in_place_60_50 set_head_angle_30_1 display_oled_face_image_face14_1 say_text_ah_1_1',\n",
       " 'set_head_angle_30_1 display_oled_face_image_face12_1 say_text_wow!_1_1 display_oled_face_image_face8_1 set_head_angle_0_0.2 display_oled_face_image_face12_1',\n",
       " 'turn_in_place_30_500 set_lift_height_1_0.2 set_lift_height_0_0.2 say_text_no!_1_0 display_oled_face_image_face6_1',\n",
       " 'display_oled_face_image_face1_1 say_text_Uhh_1_1 set_head_angle_-10_1 say_text_Uhh_1_0 display_oled_face_image_face5_1 display_oled_face_image_face7_1 say_text_Hmm?_1_1',\n",
       " 'drive_straight_100_100 display_oled_face_image_face7_1 say_text_ack!_1_1 display_oled_face_image_face2_1 say_text_gurr!_1_1 display_oled_face_image_face3_1 turn_in_place_30_500 turn_in_place_-60_500 turn_in_place_30_500',\n",
       " 'turn_in_place_90_200 set_head_angle_20_1 turn_in_place_-180_200 set_head_angle_0_1 turn_in_place_90_200 display_oled_face_image_face7_1',\n",
       " 'turn_in_place_60_500 display_oled_face_image_face8_1 say_text_ah_1_1 display_oled_face_image_face9_1 say_text_huh_1_0 set_head_angle_-30_1',\n",
       " 'drive_straight_50_500 set_head_angle_45_0.2 display_oled_face_image_face7_1 say_text_wow!_1_1 display_oled_face_image_face7_1',\n",
       " 'display_oled_face_image_face10_1 say_text_uh_1_0 turn_in_place_-60_100 display_oled_face_image_face10_1 turn_in_place_60_100 say_text_hmm_1_0 display_oled_face_image_face10_1',\n",
       " 'drive_straight_-100_500 display_oled_face_image_face7_1 say_text_wow!_1_1 set_lift_height_1_0.2 set_lift_height_0_0.2 set_head_angle_45_0.2 display_oled_face_image_face7_1 set_head_angle_0_0.2',\n",
       " 'display_oled_face_image_face18_1 say_text_huh_1_0 turn_in_place_-30_100 display_oled_face_image_face6_1 say_text_ew!_1_1 set_head_angle_-30_1 say_text_blAAAaaaAAA!_1_1 display_oled_face_image_face6_1',\n",
       " 'turn_in_place_-180_500 set_lift_height_0_0.2 set_lift_height_0.5_0.2 set_lift_height_0_0.2 turn_in_place_180_500 display_oled_face_image_face6_1 say_text_no!_1_1',\n",
       " 'set_head_angle_45_1 display_oled_face_image_face7_1 say_text_ah_1_1 display_oled_face_image_face8_1 say_text_ah!_1_1 set_head_angle_0_1 say_text_ok ok_1_1 display_oled_face_image_face8_1',\n",
       " 'display_oled_face_image_face4_1 say_text_hmm?_1_1 drive_straight_100_50 set_head_angle_30_1 display_oled_face_image_face8_1 say_text_ah_1_1 display_oled_face_image_face7_1 say_text_ee!_1_1',\n",
       " 'set_head_angle_30_1 display_oled_face_image_face2_1 set_head_angle_10_1 display_oled_face_image_face3_1 set_lift_height_1_0.2 say_text_grah!_1_1 set_lift_height_0_0.2 display_oled_face_image_face3_1 set_lift_height_1_0.2 set_lift_height_0_0.2',\n",
       " 'drive_straight_50_500 display_oled_face_image_face7_1 set_head_angle_30_0.2 set_lift_height_0.7_0.2 say_text_ack!_1_1 display_oled_face_image_face7_0.5 display_oled_face_image_face16_0.5 display_oled_face_image_face7_1 say_text_ack!_1_1',\n",
       " 'set_head_angle_45_0.2 display_oled_face_image_face7_1 say_text_huh?_1_1 set_head_angle_20_0.5 display_oled_face_image_face7_1 say_text_hum?_1_1',\n",
       " 'display_oled_face_image_face6_1 say_text_wAaaAAaa_1_1 set_head_angle_-30_1 say_text_wAaaAAaa_1_0 set_lift_height_1_0.2 set_lift_height_0_1 say_text_waAaaAA_1_1 set_lift_height_1_0.2 set_lift_height_0_1 set_head_angle_0_1 display_oled_face_image_face6_1',\n",
       " 'display_oled_face_image_face2_1 say_text_grah!_1_0 set_lift_height_0.5_0.2 set_head_angle_-15_0.2 display_oled_face_image_face3_1 say_text_huff!_1_0',\n",
       " 'turn_in_place_180_500 display_oled_face_image_face7_1 say_text_uh_1_0 display_oled_face_image_face6_1 say_text_oh blech_1_0 display_oled_face_image_face6_1 set_head_angle_-30_1 say_text_blaAAaaa_1_0',\n",
       " 'set_head_angle_30_0.2 display_oled_face_image_face7_1 say_text_oh? oh?_1_1 set_head_angle_0_0.2 say_text_hmm!_1_1 drive_straight_100_200 display_oled_face_image_face7_1 say_text_huh?_1_0',\n",
       " 'set_head_angle_30_1 display_oled_face_image_face18_0.5 display_oled_face_image_face17_0.5 display_oled_face_image_face4_1 drive_straight_100_100 set_head_angle_30_1 set_lift_height_1_1 set_lift_height_0_1 display_oled_face_image_face7_1',\n",
       " 'display_oled_face_image_face8_1 set_lift_height_0.5_0.2 set_lift_height_0_0.2 set_lift_height_0.5_0.2 turn_in_place_-180_500 set_lift_height_0.5_0.2 set_lift_height_0_0.2 set_lift_height_0.5_0.2 set_lift_height_0_0.2 turn_in_place_180_500 set_lift_height_1_0.2 display_oled_face_image_face8_1 say_text_yay!_1_1',\n",
       " 'display_oled_face_image_face1_1 turn_in_place_60_50 display_oled_face_image_face5_1 say_text_hmm_1_0 turn_in_place_-60_50 display_oled_face_image_face5_0.5 display_oled_face_image_face14_1 say_text_yawn_1_1 display_oled_face_image_face9_1',\n",
       " 'set_head_angle_45_2 display_oled_face_image_face10_1 say_text_huh?_1_1 set_head_angle_0_0.2',\n",
       " 'display_oled_face_image_face8_1 set_lift_height_1_0.2 say_text_Woo!_1_1 turn_in_place_360_500 set_lift_height_0_0.2 display_oled_face_image_face8_1',\n",
       " 'set_head_angle_30_1 say_text_ah!_1_1 set_lift_height_0.5_0.2 display_oled_face_image_face12_1 say_text_goo!_1_1 display_oled_face_image_face12_1',\n",
       " 'set_head_angle_20_0.2 display_oled_face_image_face7_1 say_text_oh!_1_1 set_head_angle_45_0.2 display_oled_face_image_face8_1 say_text_yay!_1_1 turn_in_place_60_500 display_oled_face_image_face8_1',\n",
       " 'say_text_Oh!?_1_1 turn_in_place_360_200 turn_in_place_-360_200 display_oled_face_image_face6_1 drive_straight_-100_160 set_head_angle_-30_1',\n",
       " 'display_oled_face_image_face14_1 say_text_hmm_1_0 turn_in_place_30_500 display_oled_face_image_face2_1 say_text_gah!_1_1 display_oled_face_image_face3_1',\n",
       " 'set_head_angle_45_0.2 set_lift_height_1_0.2 turn_in_place_60_500 display_oled_face_image_face8_2 turn_in_place_-60_500 turn_in_place_60_500 display_oled_face_image_face11_1 say_text_Yay!_1_1 set_lift_height_0_0.2',\n",
       " 'turn_in_place_30_500 display_oled_face_image_face2_1 say_text_gurr!_1_1 display_oled_face_image_face3_1 turn_in_place_-30_500 display_oled_face_image_face3_1',\n",
       " 'display_oled_face_image_face7_1 say_text_ah_1_1 drive_straight_100_200 set_head_angle_30_0.2 display_oled_face_image_face6_1 set_head_angle_-30_0.2 say_text_boo hoo_1_0 display_oled_face_image_face6_1',\n",
       " 'display_oled_face_image_face8_1 display_oled_face_image_face9_0.2 set_head_angle_-30_1 display_oled_face_image_face9_0.2 set_head_angle_0_1 display_oled_face_image_face9_0.2 display_oled_face_image_face8_1 set_lift_height_0.5_1 display_oled_face_image_face8_1 say_text_ah!_1_1',\n",
       " 'display_oled_face_image_face8_1 say_text_Uh huh!_1_1 set_lift_height_0.5_0.2 drive_straight_-30_250 say_text_Uh huh!_1_1 drive_straight_-30_250 say_text_Wahooo!_1_1 set_head_angle_30_0.5 set_lift_height_0_0.2 drive_straight_-60_30',\n",
       " 'turn_in_place_-30_100 display_oled_face_image_face7_1 say_text_ack!_1_1 display_oled_face_image_face2_1 say_text_ugh!_1_1 set_lift_height_0.5_0.2 display_oled_face_image_face14_1 say_text_gurr!_1_1 turn_in_place_30_500 display_oled_face_image_face14_1 say_text_ugh_1_0',\n",
       " 'display_oled_face_image_face7_1 display_oled_face_image_face8_1 say_text_ah!_1_1 drive_straight_60_500 set_lift_height_0.5_0.2 set_head_angle_-30_1 display_oled_face_image_face8_1 set_head_angle_0_1 say_text_ah!_1_1',\n",
       " 'drive_straight_100_100 display_oled_face_image_face7_1 say_text_wow!_1_1 display_oled_face_image_face7_1 display_oled_face_image_face1_1',\n",
       " 'say_text_Woah!_1_1 display_oled_face_image_face7_1 turn_in_place_30_90 turn_in_place_-30_90 set_lift_height_1_0.5 set_lift_height_0_0.5',\n",
       " 'display_oled_face_image_face4_1 say_text_huh?_1_1 set_head_angle_45_1 say_text_ah!_1_1 set_lift_height_0.5_1 say_text_oo!_1_1 display_oled_face_image_face8_1',\n",
       " 'display_oled_face_image_face7_1 drive_straight_-100_500 say_text_ee!_1_1 display_oled_face_image_face8_1 drive_straight_100_500 set_head_angle_30_0.2 display_oled_face_image_face8_1',\n",
       " 'display_oled_face_image_face7_1 set_head_angle_30_0.2 say_text_ack!_1_1 set_lift_height_1_0.2 set_lift_height_0_0.2 set_head_angle_0_0.2 turn_in_place_30_500 display_oled_face_image_face7_1 turn_in_place_-60_500 display_oled_face_image_face7_1 turn_in_place_30_500 display_oled_face_image_face6_1',\n",
       " 'display_oled_face_image_face7_1 say_text_ack!_1_1 set_lift_height_1_0.2 drive_straight_-100_500 set_lift_height_0_0.2 display_oled_face_image_face16_0.5 display_oled_face_image_face7_0.5 display_oled_face_image_face6_1',\n",
       " 'display_oled_face_image_face17_1 say_text_hmm_1_0 turn_in_place_30_100 display_oled_face_image_face6_1 say_text_oh!_1_0 set_head_angle_-30_1 say_text_blAAAaaaAAA!_1_1 display_oled_face_image_face6_1',\n",
       " 'display_oled_face_image_face7_1 say_text_ack!_1_1 drive_straight_-100_500 display_oled_face_image_face6_1 say_text_ack!_1_0 display_oled_face_image_face6_1 turn_in_place_30_500 display_oled_face_image_face6_1 drive_straight_-100_500',\n",
       " 'display_oled_face_image_face8_1 set_lift_height_0.5_0.5 drive_straight_50_100 drive_straight_50_100 drive_straight_50_100 set_head_angle_30_1 display_oled_face_image_face8_1',\n",
       " 'display_oled_face_image_face8_1 set_lift_height_1_0.2 say_text_oh!_1_1 set_lift_height_0_0.2 set_lift_height_1_0.2 display_oled_face_image_face8_1 say_text_yay!_1_1 set_lift_height_0_0.2']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78d61ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['confusion_sorrow_boredom',\n",
       " 'interest_desire',\n",
       " 'anger_frustration',\n",
       " 'confusion_sorrow_boredom',\n",
       " 'understanding_gratitude_relief',\n",
       " 'confusion_sorrow_boredom',\n",
       " 'understanding_gratitude_relief',\n",
       " 'joy_hope',\n",
       " 'confusion_sorrow_boredom',\n",
       " 'disgust_surprise_alarm_fear',\n",
       " 'confusion_sorrow_boredom',\n",
       " 'anger_frustration',\n",
       " 'joy_hope',\n",
       " 'interest_desire',\n",
       " 'interest_desire',\n",
       " 'disgust_surprise_alarm_fear',\n",
       " 'confusion_sorrow_boredom',\n",
       " 'anger_frustration',\n",
       " 'disgust_surprise_alarm_fear',\n",
       " 'understanding_gratitude_relief',\n",
       " 'disgust_surprise_alarm_fear',\n",
       " 'confusion_sorrow_boredom',\n",
       " 'disgust_surprise_alarm_fear',\n",
       " 'disgust_surprise_alarm_fear',\n",
       " 'disgust_surprise_alarm_fear',\n",
       " 'understanding_gratitude_relief',\n",
       " 'interest_desire',\n",
       " 'anger_frustration',\n",
       " 'disgust_surprise_alarm_fear',\n",
       " 'confusion_sorrow_boredom',\n",
       " 'confusion_sorrow_boredom',\n",
       " 'anger_frustration',\n",
       " 'disgust_surprise_alarm_fear',\n",
       " 'confusion_sorrow_boredom',\n",
       " 'interest_desire',\n",
       " 'joy_hope',\n",
       " 'confusion_sorrow_boredom',\n",
       " 'confusion_sorrow_boredom',\n",
       " 'joy_hope',\n",
       " 'interest_desire',\n",
       " 'joy_hope',\n",
       " 'disgust_surprise_alarm_fear',\n",
       " 'anger_frustration',\n",
       " 'joy_hope',\n",
       " 'anger_frustration',\n",
       " 'confusion_sorrow_boredom',\n",
       " 'understanding_gratitude_relief',\n",
       " 'joy_hope',\n",
       " 'anger_frustration',\n",
       " 'understanding_gratitude_relief',\n",
       " 'disgust_surprise_alarm_fear',\n",
       " 'disgust_surprise_alarm_fear',\n",
       " 'interest_desire',\n",
       " 'interest_desire',\n",
       " 'disgust_surprise_alarm_fear',\n",
       " 'disgust_surprise_alarm_fear',\n",
       " 'disgust_surprise_alarm_fear',\n",
       " 'disgust_surprise_alarm_fear',\n",
       " 'understanding_gratitude_relief',\n",
       " 'joy_hope']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c323db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion label distribution in training data:\n",
      " disgust_surprise_alarm_fear       127\n",
      "confusion_sorrow_boredom           98\n",
      "anger_frustration                  67\n",
      "joy_hope                           67\n",
      "interest_desire                    62\n",
      "understanding_gratitude_relief     55\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "training_distribution = pd.Series(y_train).value_counts()\n",
    "print(\"Emotion label distribution in training data:\\n\", training_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d56d24cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('original_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2211bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMRODataset(Dataset):\n",
    "    def __init__(self, text, labels, tokenizer, max_len):\n",
    "        self.behaviors = [self.preprocess_text(t) for t in text]\n",
    "        \n",
    "        self.labels = labels  # Emotion labels as indices\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.behaviors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        behavior = self.behaviors[idx]\n",
    "        label = self.labels[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            behavior,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "        'input_ids': inputs['input_ids'].squeeze(0),  # Shape: [seq_length]\n",
    "        'attention_mask': inputs['attention_mask'].squeeze(0),  # Shape: [seq_length]\n",
    "        'targets': torch.tensor(label, dtype=torch.long)  # integer Emotion label as tensor\n",
    "    }\n",
    "        \n",
    "    def preprocess_text(self,text):\n",
    "      text = text.replace(\"-\", \" \"). replace(\"_\", \" \")\n",
    "      return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a24c9458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel, get_linear_schedule_with_warmup\n",
    "\n",
    "class RobertaClass(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(RobertaClass, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.classifier = torch.nn.Linear(self.roberta.config.hidden_size, num_classes)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        pooled_output = outputs.last_hidden_state[:, 0] \n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "            return loss, logits  # Return loss and logits for training\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92d84786",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = data['Emotions'].unique()\n",
    "emotion_to_label = {emotion: idx for idx, emotion in enumerate(emotions)}\n",
    "label_to_emotion = {idx: emotion for emotion, idx in emotion_to_label.items()}\n",
    "emro_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba6e6758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger_frustration': 0,\n",
       " 'confusion_sorrow_boredom': 1,\n",
       " 'disgust_surprise_alarm_fear': 2,\n",
       " 'interest_desire': 3,\n",
       " 'joy_hope': 4,\n",
       " 'understanding_gratitude_relief': 5}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72fb6eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "max_len = 128\n",
    "batch_size = 8\n",
    "emro_train_dataset = EMRODataset(X_train, [emotion_to_label[e] for e in y_train], emro_tokenizer, max_len)\n",
    "emro_val_dataset = EMRODataset(X_dev, [emotion_to_label[e] for e in y_dev], emro_tokenizer, max_len)\n",
    "emro_train_loader = DataLoader(emro_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "emro_val_loader = DataLoader(emro_val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88bcaedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of emotions labels : 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaClass(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "num_class=len(data['Emotions'].unique())\n",
    "print(\"Total number of emotions labels :\",num_class)\n",
    "\n",
    "NUM_OUT = num_class\n",
    "emro_model = RobertaClass(NUM_OUT)\n",
    "emro_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07c93761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_emro_model(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(dataloader, desc=\"Training EMRO\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['targets'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss, logits = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49638daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_emro_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating EMRO\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['targets'].to(device)\n",
    "\n",
    "            loss, logits = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels  # Provide labels to compute loss\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    return average_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aff02f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer for EMRO\n",
    "emro_optimizer = torch.optim.AdamW(emro_model.parameters(), lr=2e-5)\n",
    "best_val_loss = float('inf')\n",
    "patience = 2  # Number of epochs to wait before stopping\n",
    "counter = 0\n",
    "# Number of epochs\n",
    "emro_epochs = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f38ea6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMRO Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training EMRO: 100%|██████████| 60/60 [00:08<00:00,  7.44it/s]\n",
      "Evaluating EMRO: 100%|██████████| 4/4 [00:00<00:00, 15.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMRO Training Loss: 1.7756\n",
      "EMRO Validation Loss: 1.7284, Accuracy: 0.3333\n",
      "EMRO Epoch 2/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training EMRO: 100%|██████████| 60/60 [00:07<00:00,  7.58it/s]\n",
      "Evaluating EMRO: 100%|██████████| 4/4 [00:00<00:00, 15.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMRO Training Loss: 1.3726\n",
      "EMRO Validation Loss: 0.8812, Accuracy: 0.6833\n",
      "EMRO Epoch 3/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training EMRO: 100%|██████████| 60/60 [00:07<00:00,  7.58it/s]\n",
      "Evaluating EMRO: 100%|██████████| 4/4 [00:00<00:00, 15.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMRO Training Loss: 0.8011\n",
      "EMRO Validation Loss: 0.5092, Accuracy: 0.8833\n",
      "EMRO Epoch 4/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training EMRO: 100%|██████████| 60/60 [00:07<00:00,  7.60it/s]\n",
      "Evaluating EMRO: 100%|██████████| 4/4 [00:00<00:00, 15.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMRO Training Loss: 0.5216\n",
      "EMRO Validation Loss: 0.4579, Accuracy: 0.8333\n",
      "EMRO Epoch 5/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training EMRO: 100%|██████████| 60/60 [00:07<00:00,  7.60it/s]\n",
      "Evaluating EMRO: 100%|██████████| 4/4 [00:00<00:00, 15.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMRO Training Loss: 0.3316\n",
      "EMRO Validation Loss: 0.3756, Accuracy: 0.9000\n",
      "EMRO Epoch 6/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training EMRO: 100%|██████████| 60/60 [00:07<00:00,  7.59it/s]\n",
      "Evaluating EMRO: 100%|██████████| 4/4 [00:00<00:00, 15.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMRO Training Loss: 0.2954\n",
      "EMRO Validation Loss: 0.4800, Accuracy: 0.8500\n",
      "EMRO Epoch 7/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training EMRO: 100%|██████████| 60/60 [00:07<00:00,  7.60it/s]\n",
      "Evaluating EMRO: 100%|██████████| 4/4 [00:00<00:00, 15.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMRO Training Loss: 0.2139\n",
      "EMRO Validation Loss: 0.4697, Accuracy: 0.8833\n",
      "Early stopping triggered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(emro_epochs):\n",
    "    print(f\"EMRO Epoch {epoch + 1}/{emro_epochs}\")\n",
    "    train_loss = train_emro_model(emro_model, emro_train_loader, emro_optimizer, device)\n",
    "    val_loss, val_accuracy = evaluate_emro_model(emro_model, emro_val_loader, device)\n",
    "    print(f\"EMRO Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"EMRO Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "        # Save the model\n",
    "        #torch.save(emro_model.state_dict(), 'best_emro_model.pt')\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# Save the trained EMRO model\n",
    "#torch.save(emro_model.state_dict(), 'emro_model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45be4958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25632/3438546974.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  emro_model.load_state_dict(torch.load('best_emro_model.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaClass(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emro_model.load_state_dict(torch.load('best_emro_model.pt'))\n",
    "emro_model.to(device)\n",
    "emro_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79ae57aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating EMRO: 100%|██████████| 8/8 [00:02<00:00,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMRO Test Loss: 0.2392, Accuracy: 0.9333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# After training is complete\n",
    "emro_test_dataset = EMRODataset(X_test, [emotion_to_label[e] for e in y_test], emro_tokenizer, max_len)\n",
    "emro_test_loader = DataLoader(emro_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loss, test_accuracy = evaluate_emro_model(emro_model, emro_test_loader, device)\n",
    "print(f\"EMRO Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4fc98e57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 26.86it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(emro_test_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['targets'].to(device)\n",
    "        _, logits = emro_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=emotion_to_label.keys(), yticklabels=emotion_to_label.keys())\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28780a45",
   "metadata": {},
   "source": [
    "**For GRED Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e125de12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "#del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6694801",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GREDDataset(Dataset):\n",
    "    def __init__(self, emotions, behaviors, tokenizer, max_len, emotion_to_label):\n",
    "        self.emotions = emotions\n",
    "        self.behaviors = behaviors\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.emotion_to_label = emotion_to_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emotions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        emotion = self.emotions[idx]\n",
    "        behavior = self.behaviors[idx]\n",
    "\n",
    "        # Create the prompt\n",
    "        prompt = f\"<|startoftext|>Emotion: {emotion} <|endoftext|> Behaviors:\"\n",
    "        target = f\"{behavior} <|endoftext|>\"\n",
    "        full_input = prompt + \" \" + target\n",
    "        \n",
    "        # Tokenize the full input\n",
    "        inputs = self.tokenizer(\n",
    "            full_input,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = inputs['input_ids'].squeeze()  # Shape: (seq_len,)\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "        \n",
    "        labels = input_ids.clone()\n",
    "        labels[:len(self.tokenizer(prompt)['input_ids'])] = -100 \n",
    "        emotion_label = torch.tensor(self.emotion_to_label[emotion], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "            'emotion_label': emotion_label  # Integer emotion label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "215ec247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "gred_tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-medium\")\n",
    "gred_model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b023da9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50259, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50259, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a padding token and resize token embeddings\n",
    "special_tokens_dict = {'additional_special_tokens': ['<|startoftext|>', '<|endoftext|>']}\n",
    "gred_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "if gred_tokenizer.pad_token is None:\n",
    "    gred_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "gred_model.resize_token_embeddings(len(gred_tokenizer))\n",
    "gred_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac508009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gred(gred_model, dataloader, optimizer, device):\n",
    "    gred_model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        # Move data to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        emotion_labels = batch['emotion_label'].to(device)  # Original emotion label for GRED\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        outputs = gred_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "145ca8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gred(gred_model, dataloader, device):\n",
    "    gred_model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move data to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            emotion_labels = batch['emotion_label'].to(device)\n",
    "    \n",
    "            # Forward pass through GRED model with EMRO feedback\n",
    "            outputs = gred_model(input_ids, attention_mask=attention_mask, labels = labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c169d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "gred_train_dataset = GREDDataset(y_train, X_train, gred_tokenizer, max_len, emotion_to_label)\n",
    "gred_val_dataset = GREDDataset(y_dev, X_dev, gred_tokenizer, max_len, emotion_to_label)\n",
    "gred_test_dataset = GREDDataset(y_test, X_test, gred_tokenizer, max_len, emotion_to_label)\n",
    "train_batch_size = 8\n",
    "val_batch_size =16\n",
    "\n",
    "gred_train_loader = DataLoader(gred_train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "gred_val_loader = DataLoader(gred_val_dataset, batch_size=val_batch_size, shuffle=False)\n",
    "gred_test_loader = DataLoader(gred_test_dataset, batch_size=val_batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9efbaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "gred_optimizer = torch.optim.AdamW(gred_model.parameters(), lr=2e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a02f7a8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/18\n",
      "Training Loss: 3.3940 \n",
      "Validation Loss: 2.4562\n",
      "\n",
      "Epoch 2/18\n",
      "Training Loss: 1.9311 \n",
      "Validation Loss: 1.0602\n",
      "\n",
      "Epoch 3/18\n",
      "Training Loss: 0.6476 \n",
      "Validation Loss: 0.3788\n",
      "\n",
      "Epoch 4/18\n",
      "Training Loss: 0.3607 \n",
      "Validation Loss: 0.3526\n",
      "\n",
      "Epoch 5/18\n",
      "Training Loss: 0.3323 \n",
      "Validation Loss: 0.3328\n",
      "\n",
      "Epoch 6/18\n",
      "Training Loss: 0.3118 \n",
      "Validation Loss: 0.3229\n",
      "\n",
      "Epoch 7/18\n",
      "Training Loss: 0.3002 \n",
      "Validation Loss: 0.3240\n",
      "\n",
      "Epoch 8/18\n",
      "Training Loss: 0.2890 \n",
      "Validation Loss: 0.3186\n",
      "\n",
      "Epoch 9/18\n",
      "Training Loss: 0.2807 \n",
      "Validation Loss: 0.3181\n",
      "\n",
      "Epoch 10/18\n",
      "Training Loss: 0.2737 \n",
      "Validation Loss: 0.3195\n",
      "\n",
      "Epoch 11/18\n",
      "Training Loss: 0.2623 \n",
      "Validation Loss: 0.3155\n",
      "\n",
      "Epoch 12/18\n",
      "Training Loss: 0.2541 \n",
      "Validation Loss: 0.3225\n",
      "\n",
      "Epoch 13/18\n",
      "Training Loss: 0.2501 \n",
      "Validation Loss: 0.3259\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "patience = 2  # Number of epochs to wait before stopping\n",
    "counter = 0\n",
    "EPOCHS = 18\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    \n",
    "    # Training Phase\n",
    "    train_loss = train_gred(\n",
    "        gred_model, gred_train_loader, gred_optimizer, device\n",
    "    )\n",
    "    print(f\"Training Loss: {train_loss:.4f} \")\n",
    "    \n",
    "    val_loss= evaluate_gred(\n",
    "        gred_model, gred_val_loader, device\n",
    "    )\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "        # Save the model  \n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3dcb22dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50259, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50259, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gred_model.load_state_dict(torch.load('best_gred_model.pt', weights_only=True))\n",
    "gred_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9cd48e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(emotion):\n",
    "    prompt = f\"<|startoftext|>Emotion: {emotion} <|endoftext|> Behaviors:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a57cb36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_behavior(gred_model, gred_tokenizer, emotion, num_samples=2, max_length=256):\n",
    "    behaviors = []\n",
    "    gred_model.eval()\n",
    "    prompt = create_prompt(emotion)\n",
    "\n",
    "    input_ids = gred_tokenizer.encode(\n",
    "        prompt, \n",
    "        return_tensors='pt', \n",
    "        max_length=max_length, \n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        with torch.no_grad():\n",
    "            output_sequences = gred_model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                temperature=0.7,\n",
    "                pad_token_id=gred_tokenizer.pad_token_id\n",
    "            )\n",
    "        generated_text = gred_tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "        #print(\"generated text is \", generated_text)\n",
    "        # Remove the prompt from the generated text\n",
    "        behavior = generated_text.split(\":\")[-1].strip()\n",
    "        behaviors.append(behavior)\n",
    "    \n",
    "    return behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7399a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate behaviors for each emotion\n",
    "def generate_behaviors_for_emotions(gred_model, gred_tokenizer, emotions, num_samples=20, num_runs=10):\n",
    "    all_generated_behaviors = []\n",
    "    emotions_list = []\n",
    "    behaviors_list = []\n",
    "\n",
    "    for _ in range(num_runs):  # Repeat the process for 10 runs\n",
    "        for emotion in emotions:\n",
    "            generated_behaviors = generate_behavior(\n",
    "                gred_model=gred_model,\n",
    "                gred_tokenizer=gred_tokenizer,\n",
    "                emotion=emotion,\n",
    "                num_samples=num_samples\n",
    "            )\n",
    "\n",
    "            for behavior in generated_behaviors:\n",
    "                emotions_list.append(emotion)\n",
    "                behaviors_list.append(behavior)\n",
    "                #print(f\"{emotion}: {behavior}\")\n",
    "                all_generated_behaviors.append((emotion, behavior))\n",
    "\n",
    "    return all_generated_behaviors, emotions_list, behaviors_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6fdad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_generated_behaviors, emotions_list, behaviors_list = generate_behaviors_for_emotions(\n",
    "    gred_model=gred_model,\n",
    "    gred_tokenizer=gred_tokenizer,\n",
    "    emotions=emotions,\n",
    "    num_samples=20,  # Behaviors per run\n",
    "    num_runs=5  # Total runs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "40232aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame({\"Emotion\": emotions_list, \"Behavior\": behaviors_list})\n",
    "#df.to_csv(\"gred_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "738cc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV\n",
    "#df = pd.read_csv(\"gred_data.csv\")\n",
    "\n",
    "# Convert into list of tuples (emotion, behavior)\n",
    "#all_generated_behaviors = [(row[\"Emotion\"], row[\"Behavior\"]) for _, row in df.iterrows()]\n",
    "# Extract behaviors and emotions as lists\n",
    "#behaviors_list = df[\"Behavior\"].tolist()\n",
    "#emotions_list = df[\"Emotion\"].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2653deb",
   "metadata": {},
   "source": [
    "**Evaluating using EMRO model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "011a1c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = EMRODataset(behaviors_list, [emotion_to_label[e] for e in emotions_list], emro_tokenizer, max_len)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4bbd049d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating EMRO: 100%|██████████| 75/75 [00:02<00:00, 26.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMRO Test Loss: 0.3009, Accuracy: 0.8950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# After training is complete\n",
    "test_loss, test_accuracy = evaluate_emro_model(emro_model, test_loader, device)\n",
    "print(f\"EMRO Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebaf80e",
   "metadata": {},
   "source": [
    "**Getting novelty scores for the behvaiors generated without any loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "631444aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsExtractor(torch.nn.Module):\n",
    "    def __init__(self, emro_model, tokenizer, device):\n",
    "        super().__init__()\n",
    "        self.emro_model = emro_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        for param in self.emro_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.emro_model.eval()\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "        inputs = self.tokenizer(\n",
    "            text, return_tensors='pt',  max_length=128, truncation=True, padding=True\n",
    "        ).to(self.device)\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        outputs = self.emro_model.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        # Usually take the [CLS] token (first token) as the pooled embedding\n",
    "        pooled_output = last_hidden_state[:, 0, :]\n",
    "    \n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0fd16ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "def compute_cosine_similarity_for_emotion(embeddings, emotion_labels, emotion):\n",
    "    # Extract indices of behaviors with the given emotion\n",
    "    emotion_indices = [i for i, label in enumerate(emotion_labels) if label == emotion]\n",
    "    emotion_embeddings = [embeddings[i] for i in emotion_indices]  # Extract embeddings for this emotion\n",
    "\n",
    "    # Ensure the embeddings are moved to CPU before calculating cosine similarity\n",
    "    emotion_embeddings_cpu = [embedding.cpu().numpy() for embedding in emotion_embeddings]\n",
    "    \n",
    "    # Compute the cosine similarity matrix for the emotion embeddings\n",
    "    cosine_sim_matrix = cosine_similarity(emotion_embeddings_cpu)\n",
    "    \n",
    "    # Get the upper triangular indices to compute pairwise similarity for unique pairs\n",
    "    upper_triangular_indices = np.triu_indices(len(emotion_embeddings_cpu), k=1)\n",
    "    similarity_values = cosine_sim_matrix[upper_triangular_indices]\n",
    "\n",
    "    # Calculate the average cosine similarity\n",
    "    avg_cosine_similarity = np.mean(similarity_values) if similarity_values.size > 0 else 1.0\n",
    "\n",
    "    return avg_cosine_similarity\n",
    "\n",
    "# Calculate novelty for each emotion\n",
    "def compute_average_novelty_by_emotion(embeddings, emotion_labels):\n",
    "    avg_novelty_by_emotion = {}\n",
    "    \n",
    "    # Loop through each emotion and compute average cosine similarity and novelty\n",
    "    emotions = set(emotion_labels)  # Get unique emotions\n",
    "    for emotion in emotions:\n",
    "        avg_cosine_similarity = compute_cosine_similarity_for_emotion(embeddings, emotion_labels, emotion)\n",
    "        #novelty_scores = torch.stack([torch.exp(-cs) for cs in avg_cosine_sims])\n",
    "        avg_novelty_score = 1 - avg_cosine_similarity  # Novelty is inversely related to similarity\n",
    "        avg_novelty_by_emotion[emotion] = np.float32(avg_novelty_score)  # Store novelty as float32\n",
    "    \n",
    "    return avg_novelty_by_emotion\n",
    "\n",
    "def evaluate_novelty(all_generated_behaviors, embeddings_extractor):\n",
    "    behaviors = [behavior for _, behavior in all_generated_behaviors]\n",
    "    emotion_labels = [emotion for emotion, _ in all_generated_behaviors]\n",
    "\n",
    "    # Extract embeddings for all behaviors using the embeddings extractor\n",
    "    behavior_embeddings = embeddings_extractor.get_embedding(behaviors)\n",
    "\n",
    "    # Compute average novelty score by emotion\n",
    "    avg_novelty_by_emotion = compute_average_novelty_by_emotion(behavior_embeddings, emotion_labels)\n",
    "\n",
    "    # Print or return the novelty scores\n",
    "    print(\"Novelty Scores for each emotions\")\n",
    "    for emotion, novelty_score in avg_novelty_by_emotion.items():\n",
    "        print(f\"{emotion}:: {novelty_score}\")\n",
    "\n",
    "    return avg_novelty_by_emotion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cecbcee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novelty Scores for each emotions\n",
      "interest_desire:: 0.36489301919937134\n",
      "anger_frustration:: 0.06581497192382812\n",
      "disgust_surprise_alarm_fear:: 0.29562056064605713\n",
      "confusion_sorrow_boredom:: 0.37712782621383667\n",
      "joy_hope:: 0.1551913619041443\n",
      "understanding_gratitude_relief:: 0.25257617235183716\n"
     ]
    }
   ],
   "source": [
    "embeddings_extractor = EmbeddingsExtractor(emro_model, emro_tokenizer, device)\n",
    "avg_novelty_by_emotion = evaluate_novelty(all_generated_behaviors, embeddings_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d96fe7f",
   "metadata": {},
   "source": [
    "**Novelty scores for original behaviors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8d77461d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Novelty Scores:\n",
      "confusion_sorrow_boredom: 0.39123237133026123\n",
      "disgust_surprise_alarm_fear: 0.28924643993377686\n",
      "joy_hope: 0.31836092472076416\n",
      "anger_frustration: 0.20455330610275269\n",
      "interest_desire: 0.36509305238723755\n",
      "understanding_gratitude_relief: 0.44749927520751953\n"
     ]
    }
   ],
   "source": [
    "original_embeddings = embeddings_extractor.get_embedding(texts)\n",
    "original_novelty_by_emotion = compute_average_novelty_by_emotion(original_embeddings, labels)\n",
    "\n",
    "print(\"Original Novelty Scores:\")\n",
    "for emotion, novelty_score in original_novelty_by_emotion.items():\n",
    "    print(f\"{emotion}: {novelty_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac22ef1",
   "metadata": {},
   "source": [
    "**GRED with EMRO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a1d4b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50259, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50259, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gred_model.load_state_dict(torch.load('best_gred_model.pt', weights_only=True))\n",
    "gred_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c94659c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Model\n",
    "class GREDwithEMRO(nn.Module):\n",
    "    def __init__(self, gred_model, gred_tokenizer, emro_model, emro_tokenizer, emotion_to_label, max_len=128):\n",
    "        super(GREDwithEMRO, self).__init__()\n",
    "        self.gred_model = gred_model\n",
    "        self.gred_tokenizer = gred_tokenizer\n",
    "        self.emro_model = emro_model\n",
    "        self.emro_tokenizer = emro_tokenizer\n",
    "        self.emro_model.eval()\n",
    "        for param in self.emro_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.gred_model.config.output_hidden_states = True\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.gred_model.config.hidden_size, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 768),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.emro_loss_fn = nn.CrossEntropyLoss()\n",
    "        self.emotion_to_label = emotion_to_label\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels, emotion_label):\n",
    "        gred_outputs = self.gred_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        gred_loss = gred_outputs.loss\n",
    "        hidden_states = gred_outputs.hidden_states[-1]\n",
    "        projected_embeddings = self.projection(hidden_states).mean(dim=1)\n",
    "        emro_logits = self.emro_model.classifier(self.emro_model.dropout(projected_embeddings))\n",
    "        emro_loss = self.emro_loss_fn(emro_logits, emotion_label)\n",
    "        total_loss = gred_loss + emro_loss\n",
    "        return total_loss, gred_loss, emro_loss\n",
    "\n",
    "# Training and Evaluation Functions\n",
    "def train_gred_with_emro_feedback(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_gred_loss = 0.0\n",
    "    total_emro_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        emotion_labels = batch['emotion_label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        total_batch_loss, gred_loss, emro_loss = model(input_ids, attention_mask, labels, emotion_labels)\n",
    "        \n",
    "        total_batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += total_batch_loss.item()\n",
    "        total_gred_loss += gred_loss.item()\n",
    "        total_emro_loss += emro_loss.item()\n",
    "        \n",
    "    average_total_loss = total_loss / len(dataloader)\n",
    "    average_gred_loss = total_gred_loss / len(dataloader)\n",
    "    average_emro_loss = total_emro_loss / len(dataloader)\n",
    "    \n",
    "    return average_total_loss,average_gred_loss, average_emro_loss\n",
    "\n",
    "# Data Preparation and Training Loop\n",
    "gred_train_dataset = GREDDataset(y_train, X_train, gred_tokenizer, max_len, emotion_to_label)\n",
    "gred_train_loader = DataLoader(gred_train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "gred_model_name = GREDwithEMRO(gred_model, gred_tokenizer, emro_model, emro_tokenizer, emotion_to_label).to(device)\n",
    "gred_optimizer = torch.optim.AdamW(\n",
    "    list(gred_model_name.gred_model.parameters()) + list(gred_model_name.projection.parameters()), \n",
    "    lr=1e-5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a79bf972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/18\n",
      "Training Loss: 2.4023 | GRED Loss: 0.2676 | EMRO Loss: 1.7789\n",
      "\n",
      "Epoch 2/18\n",
      "Training Loss: 1.4920 | GRED Loss: 0.3180 | EMRO Loss: 0.9784\n",
      "\n",
      "Epoch 3/18\n",
      "Training Loss: 0.6337 | GRED Loss: 0.3304 | EMRO Loss: 0.2528\n",
      "\n",
      "Epoch 4/18\n",
      "Training Loss: 0.3771 | GRED Loss: 0.2991 | EMRO Loss: 0.0650\n",
      "\n",
      "Epoch 5/18\n",
      "Training Loss: 0.3173 | GRED Loss: 0.2810 | EMRO Loss: 0.0302\n",
      "\n",
      "Epoch 6/18\n",
      "Training Loss: 0.2937 | GRED Loss: 0.2718 | EMRO Loss: 0.0183\n",
      "\n",
      "Epoch 7/18\n",
      "Training Loss: 0.2772 | GRED Loss: 0.2615 | EMRO Loss: 0.0131\n",
      "\n",
      "Epoch 8/18\n",
      "Training Loss: 0.2671 | GRED Loss: 0.2548 | EMRO Loss: 0.0103\n",
      "\n",
      "Epoch 9/18\n",
      "Training Loss: 0.2605 | GRED Loss: 0.2514 | EMRO Loss: 0.0075\n",
      "\n",
      "Epoch 10/18\n",
      "Training Loss: 0.2546 | GRED Loss: 0.2473 | EMRO Loss: 0.0061\n",
      "\n",
      "Epoch 11/18\n",
      "Training Loss: 0.2484 | GRED Loss: 0.2421 | EMRO Loss: 0.0053\n",
      "\n",
      "Epoch 12/18\n",
      "Training Loss: 0.2438 | GRED Loss: 0.2382 | EMRO Loss: 0.0046\n",
      "\n",
      "Epoch 13/18\n",
      "Training Loss: 0.2390 | GRED Loss: 0.2347 | EMRO Loss: 0.0036\n",
      "\n",
      "Epoch 14/18\n",
      "Training Loss: 0.2343 | GRED Loss: 0.2302 | EMRO Loss: 0.0034\n",
      "\n",
      "Epoch 15/18\n",
      "Training Loss: 0.2300 | GRED Loss: 0.2266 | EMRO Loss: 0.0029\n",
      "\n",
      "Epoch 16/18\n",
      "Training Loss: 0.2265 | GRED Loss: 0.2234 | EMRO Loss: 0.0026\n",
      "\n",
      "Epoch 17/18\n",
      "Training Loss: 0.2206 | GRED Loss: 0.2179 | EMRO Loss: 0.0023\n",
      "\n",
      "Epoch 18/18\n",
      "Training Loss: 0.2186 | GRED Loss: 0.2162 | EMRO Loss: 0.0020\n"
     ]
    }
   ],
   "source": [
    "EPOCHS= 18\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    train_loss, train_gred_loss, train_emro_loss = train_gred_with_emro_feedback(gred_model_name, gred_train_loader, gred_optimizer, device)\n",
    "    print(f\"Training Loss: {train_loss:.4f} | GRED Loss: {train_gred_loss:.4f} | EMRO Loss: {train_emro_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efb26a9",
   "metadata": {},
   "source": [
    "**Checking the gradients after training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e5734a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for gred_model.transformer.wte.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.wpe.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.0.ln_1.weight - Mean: -0.0003\n",
      "Gradient for gred_model.transformer.h.0.ln_1.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.0.attn.c_attn.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.0.attn.c_attn.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.0.attn.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.0.attn.c_proj.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.0.ln_2.weight - Mean: -0.0002\n",
      "Gradient for gred_model.transformer.h.0.ln_2.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.0.mlp.c_fc.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.0.mlp.c_fc.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.0.mlp.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.0.mlp.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.1.ln_1.weight - Mean: 0.0002\n",
      "Gradient for gred_model.transformer.h.1.ln_1.bias - Mean: 0.0008\n",
      "Gradient for gred_model.transformer.h.1.attn.c_attn.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.1.attn.c_attn.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.1.attn.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.1.attn.c_proj.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.1.ln_2.weight - Mean: 0.0003\n",
      "Gradient for gred_model.transformer.h.1.ln_2.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.1.mlp.c_fc.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.1.mlp.c_fc.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.1.mlp.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.1.mlp.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.2.ln_1.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.2.ln_1.bias - Mean: 0.0004\n",
      "Gradient for gred_model.transformer.h.2.attn.c_attn.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.2.attn.c_attn.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.2.attn.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.2.attn.c_proj.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.2.ln_2.weight - Mean: 0.0001\n",
      "Gradient for gred_model.transformer.h.2.ln_2.bias - Mean: 0.0003\n",
      "Gradient for gred_model.transformer.h.2.mlp.c_fc.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.2.mlp.c_fc.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.2.mlp.c_proj.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.2.mlp.c_proj.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.3.ln_1.weight - Mean: 0.0001\n",
      "Gradient for gred_model.transformer.h.3.ln_1.bias - Mean: -0.0001\n",
      "Gradient for gred_model.transformer.h.3.attn.c_attn.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.3.attn.c_attn.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.3.attn.c_proj.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.3.attn.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.3.ln_2.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.3.ln_2.bias - Mean: -0.0001\n",
      "Gradient for gred_model.transformer.h.3.mlp.c_fc.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.3.mlp.c_fc.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.3.mlp.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.3.mlp.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.4.ln_1.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.4.ln_1.bias - Mean: -0.0003\n",
      "Gradient for gred_model.transformer.h.4.attn.c_attn.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.4.attn.c_attn.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.4.attn.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.4.attn.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.4.ln_2.weight - Mean: -0.0001\n",
      "Gradient for gred_model.transformer.h.4.ln_2.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.4.mlp.c_fc.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.4.mlp.c_fc.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.4.mlp.c_proj.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.4.mlp.c_proj.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.5.ln_1.weight - Mean: -0.0001\n",
      "Gradient for gred_model.transformer.h.5.ln_1.bias - Mean: 0.0001\n",
      "Gradient for gred_model.transformer.h.5.attn.c_attn.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.5.attn.c_attn.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.5.attn.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.5.attn.c_proj.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.5.ln_2.weight - Mean: -0.0004\n",
      "Gradient for gred_model.transformer.h.5.ln_2.bias - Mean: -0.0001\n",
      "Gradient for gred_model.transformer.h.5.mlp.c_fc.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.5.mlp.c_fc.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.5.mlp.c_proj.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.5.mlp.c_proj.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.6.ln_1.weight - Mean: -0.0001\n",
      "Gradient for gred_model.transformer.h.6.ln_1.bias - Mean: 0.0002\n",
      "Gradient for gred_model.transformer.h.6.attn.c_attn.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.6.attn.c_attn.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.6.attn.c_proj.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.6.attn.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.6.ln_2.weight - Mean: -0.0001\n",
      "Gradient for gred_model.transformer.h.6.ln_2.bias - Mean: 0.0001\n",
      "Gradient for gred_model.transformer.h.6.mlp.c_fc.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.6.mlp.c_fc.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.6.mlp.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.6.mlp.c_proj.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.7.ln_1.weight - Mean: -0.0002\n",
      "Gradient for gred_model.transformer.h.7.ln_1.bias - Mean: 0.0001\n",
      "Gradient for gred_model.transformer.h.7.attn.c_attn.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.7.attn.c_attn.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.7.attn.c_proj.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.7.attn.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.7.ln_2.weight - Mean: -0.0001\n",
      "Gradient for gred_model.transformer.h.7.ln_2.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.7.mlp.c_fc.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.7.mlp.c_fc.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.7.mlp.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.7.mlp.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.8.ln_1.weight - Mean: 0.0001\n",
      "Gradient for gred_model.transformer.h.8.ln_1.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.8.attn.c_attn.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.8.attn.c_attn.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.8.attn.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.8.attn.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.8.ln_2.weight - Mean: 0.0001\n",
      "Gradient for gred_model.transformer.h.8.ln_2.bias - Mean: 0.0001\n",
      "Gradient for gred_model.transformer.h.8.mlp.c_fc.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.8.mlp.c_fc.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.8.mlp.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.8.mlp.c_proj.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.9.ln_1.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.9.ln_1.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.9.attn.c_attn.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.9.attn.c_attn.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.9.attn.c_proj.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.9.attn.c_proj.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.9.ln_2.weight - Mean: 0.0001\n",
      "Gradient for gred_model.transformer.h.9.ln_2.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.9.mlp.c_fc.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.9.mlp.c_fc.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.9.mlp.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.9.mlp.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.10.ln_1.weight - Mean: -0.0001\n",
      "Gradient for gred_model.transformer.h.10.ln_1.bias - Mean: -0.0001\n",
      "Gradient for gred_model.transformer.h.10.attn.c_attn.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.10.attn.c_attn.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.10.attn.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.10.attn.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.10.ln_2.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.10.ln_2.bias - Mean: -0.0001\n",
      "Gradient for gred_model.transformer.h.10.mlp.c_fc.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.10.mlp.c_fc.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.10.mlp.c_proj.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.10.mlp.c_proj.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.11.ln_1.weight - Mean: -0.0001\n",
      "Gradient for gred_model.transformer.h.11.ln_1.bias - Mean: 0.0001\n",
      "Gradient for gred_model.transformer.h.11.attn.c_attn.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.11.attn.c_attn.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.11.attn.c_proj.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.11.attn.c_proj.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.11.ln_2.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.11.ln_2.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.11.mlp.c_fc.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.11.mlp.c_fc.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.11.mlp.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.11.mlp.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.12.ln_1.weight - Mean: -0.0001\n",
      "Gradient for gred_model.transformer.h.12.ln_1.bias - Mean: 0.0004\n",
      "Gradient for gred_model.transformer.h.12.attn.c_attn.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.12.attn.c_attn.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.12.attn.c_proj.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.12.attn.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.12.ln_2.weight - Mean: -0.0001\n",
      "Gradient for gred_model.transformer.h.12.ln_2.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.12.mlp.c_fc.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.12.mlp.c_fc.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.12.mlp.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.12.mlp.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.13.ln_1.weight - Mean: -0.0001\n",
      "Gradient for gred_model.transformer.h.13.ln_1.bias - Mean: 0.0001\n",
      "Gradient for gred_model.transformer.h.13.attn.c_attn.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.13.attn.c_attn.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.13.attn.c_proj.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.13.attn.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.13.ln_2.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.13.ln_2.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.13.mlp.c_fc.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.13.mlp.c_fc.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.13.mlp.c_proj.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.13.mlp.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.14.ln_1.weight - Mean: -0.0001\n",
      "Gradient for gred_model.transformer.h.14.ln_1.bias - Mean: 0.0002\n",
      "Gradient for gred_model.transformer.h.14.attn.c_attn.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.14.attn.c_attn.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.14.attn.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.14.attn.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.14.ln_2.weight - Mean: 0.0001\n",
      "Gradient for gred_model.transformer.h.14.ln_2.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.14.mlp.c_fc.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.14.mlp.c_fc.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.14.mlp.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.14.mlp.c_proj.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.15.ln_1.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.15.ln_1.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.15.attn.c_attn.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.15.attn.c_attn.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.15.attn.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.15.attn.c_proj.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.15.ln_2.weight - Mean: 0.0002\n",
      "Gradient for gred_model.transformer.h.15.ln_2.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.15.mlp.c_fc.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.15.mlp.c_fc.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.15.mlp.c_proj.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.15.mlp.c_proj.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.16.ln_1.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.16.ln_1.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.16.attn.c_attn.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.16.attn.c_attn.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.16.attn.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.16.attn.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.16.ln_2.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.16.ln_2.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.16.mlp.c_fc.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.16.mlp.c_fc.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.16.mlp.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.16.mlp.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.17.ln_1.weight - Mean: 0.0001\n",
      "Gradient for gred_model.transformer.h.17.ln_1.bias - Mean: -0.0002\n",
      "Gradient for gred_model.transformer.h.17.attn.c_attn.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.17.attn.c_attn.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.17.attn.c_proj.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.17.attn.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.17.ln_2.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.17.ln_2.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.17.mlp.c_fc.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.17.mlp.c_fc.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.17.mlp.c_proj.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.17.mlp.c_proj.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.18.ln_1.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.18.ln_1.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.18.attn.c_attn.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.18.attn.c_attn.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.18.attn.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.18.attn.c_proj.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.18.ln_2.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.18.ln_2.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.18.mlp.c_fc.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.18.mlp.c_fc.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.18.mlp.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.18.mlp.c_proj.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.19.ln_1.weight - Mean: 0.0001\n",
      "Gradient for gred_model.transformer.h.19.ln_1.bias - Mean: -0.0001\n",
      "Gradient for gred_model.transformer.h.19.attn.c_attn.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.19.attn.c_attn.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.19.attn.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.19.attn.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.19.ln_2.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.19.ln_2.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.19.mlp.c_fc.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.19.mlp.c_fc.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.19.mlp.c_proj.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.19.mlp.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.20.ln_1.weight - Mean: -0.0001\n",
      "Gradient for gred_model.transformer.h.20.ln_1.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.20.attn.c_attn.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.20.attn.c_attn.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.20.attn.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.20.attn.c_proj.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.20.ln_2.weight - Mean: -0.0001\n",
      "Gradient for gred_model.transformer.h.20.ln_2.bias - Mean: -0.0001\n",
      "Gradient for gred_model.transformer.h.20.mlp.c_fc.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.20.mlp.c_fc.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.20.mlp.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.20.mlp.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.21.ln_1.weight - Mean: -0.0001\n",
      "Gradient for gred_model.transformer.h.21.ln_1.bias - Mean: 0.0003\n",
      "Gradient for gred_model.transformer.h.21.attn.c_attn.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.21.attn.c_attn.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.21.attn.c_proj.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.21.attn.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.21.ln_2.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.21.ln_2.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.21.mlp.c_fc.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.21.mlp.c_fc.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.21.mlp.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.21.mlp.c_proj.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.22.ln_1.weight - Mean: -0.0001\n",
      "Gradient for gred_model.transformer.h.22.ln_1.bias - Mean: 0.0001\n",
      "Gradient for gred_model.transformer.h.22.attn.c_attn.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.22.attn.c_attn.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.22.attn.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.22.attn.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.22.ln_2.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.22.ln_2.bias - Mean: 0.0001\n",
      "Gradient for gred_model.transformer.h.22.mlp.c_fc.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.22.mlp.c_fc.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.22.mlp.c_proj.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.22.mlp.c_proj.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.23.ln_1.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.23.ln_1.bias - Mean: -0.0003\n",
      "Gradient for gred_model.transformer.h.23.attn.c_attn.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.23.attn.c_attn.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.23.attn.c_proj.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.23.attn.c_proj.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.23.ln_2.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.23.ln_2.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.23.mlp.c_fc.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.23.mlp.c_fc.bias - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.h.23.mlp.c_proj.weight - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.h.23.mlp.c_proj.bias - Mean: 0.0000\n",
      "Gradient for gred_model.transformer.ln_f.weight - Mean: -0.0000\n",
      "Gradient for gred_model.transformer.ln_f.bias - Mean: -0.0000\n",
      "No gradient for emro_model.roberta.embeddings.word_embeddings.weight\n",
      "No gradient for emro_model.roberta.embeddings.position_embeddings.weight\n",
      "No gradient for emro_model.roberta.embeddings.token_type_embeddings.weight\n",
      "No gradient for emro_model.roberta.embeddings.LayerNorm.weight\n",
      "No gradient for emro_model.roberta.embeddings.LayerNorm.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.0.attention.self.query.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.0.attention.self.query.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.0.attention.self.key.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.0.attention.self.key.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.0.attention.self.value.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.0.attention.self.value.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.0.attention.output.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.0.attention.output.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.0.intermediate.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.0.intermediate.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.0.output.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.0.output.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.0.output.LayerNorm.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.0.output.LayerNorm.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.1.attention.self.query.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.1.attention.self.query.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.1.attention.self.key.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.1.attention.self.key.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.1.attention.self.value.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.1.attention.self.value.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.1.attention.output.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.1.attention.output.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.1.intermediate.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.1.intermediate.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.1.output.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.1.output.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.1.output.LayerNorm.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.1.output.LayerNorm.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.2.attention.self.query.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.2.attention.self.query.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.2.attention.self.key.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.2.attention.self.key.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.2.attention.self.value.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.2.attention.self.value.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.2.attention.output.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.2.attention.output.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.2.intermediate.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.2.intermediate.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.2.output.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.2.output.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.2.output.LayerNorm.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.2.output.LayerNorm.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.3.attention.self.query.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.3.attention.self.query.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.3.attention.self.key.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.3.attention.self.key.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.3.attention.self.value.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.3.attention.self.value.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.3.attention.output.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.3.attention.output.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.3.intermediate.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.3.intermediate.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.3.output.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.3.output.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.3.output.LayerNorm.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.3.output.LayerNorm.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.4.attention.self.query.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.4.attention.self.query.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.4.attention.self.key.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.4.attention.self.key.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.4.attention.self.value.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.4.attention.self.value.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.4.attention.output.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.4.attention.output.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.4.intermediate.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.4.intermediate.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.4.output.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.4.output.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.4.output.LayerNorm.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.4.output.LayerNorm.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.5.attention.self.query.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.5.attention.self.query.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.5.attention.self.key.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.5.attention.self.key.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.5.attention.self.value.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.5.attention.self.value.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.5.attention.output.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.5.attention.output.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.5.intermediate.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.5.intermediate.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.5.output.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.5.output.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.6.attention.self.query.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.6.attention.self.query.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.6.attention.self.key.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.6.attention.self.key.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.6.attention.self.value.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.6.attention.self.value.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.6.attention.output.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.6.attention.output.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.6.intermediate.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.6.intermediate.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.6.output.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.6.output.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.6.output.LayerNorm.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.6.output.LayerNorm.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.7.attention.self.query.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.7.attention.self.query.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.7.attention.self.key.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.7.attention.self.key.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.7.attention.self.value.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.7.attention.self.value.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.7.attention.output.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.7.attention.output.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.7.intermediate.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.7.intermediate.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.7.output.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.7.output.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.8.attention.self.query.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.8.attention.self.query.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.8.attention.self.key.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.8.attention.self.key.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.8.attention.self.value.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.8.attention.self.value.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.8.attention.output.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.8.attention.output.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.8.intermediate.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.8.intermediate.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.8.output.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.8.output.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.9.attention.self.query.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.9.attention.self.query.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.9.attention.self.key.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.9.attention.self.key.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.9.attention.self.value.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.9.attention.self.value.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.9.attention.output.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.9.attention.output.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.9.intermediate.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.9.intermediate.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.9.output.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.9.output.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.10.attention.self.query.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.10.attention.self.query.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.10.attention.self.key.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.10.attention.self.key.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.10.attention.self.value.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.10.attention.self.value.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.10.attention.output.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.10.attention.output.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.10.intermediate.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.10.intermediate.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.10.output.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.10.output.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.11.attention.self.query.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.11.attention.self.query.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.11.attention.self.key.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.11.attention.self.key.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.11.attention.self.value.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.11.attention.self.value.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.11.attention.output.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.11.attention.output.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.11.intermediate.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.11.intermediate.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.11.output.dense.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.11.output.dense.bias\n",
      "No gradient for emro_model.roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "No gradient for emro_model.roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "No gradient for emro_model.roberta.pooler.dense.weight\n",
      "No gradient for emro_model.roberta.pooler.dense.bias\n",
      "No gradient for emro_model.classifier.weight\n",
      "No gradient for emro_model.classifier.bias\n",
      "Gradient for projection.0.weight - Mean: 0.0000\n",
      "Gradient for projection.0.bias - Mean: -0.0000\n",
      "Gradient for projection.2.weight - Mean: -0.0000\n",
      "Gradient for projection.2.bias - Mean: -0.0000\n"
     ]
    }
   ],
   "source": [
    "for name, param in gred_model_name.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"Gradient for {name} - Mean: {param.grad.mean():.4f}\")\n",
    "    else:\n",
    "        print(f\"No gradient for {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "15a4fbdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "#del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b029b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_generated_behaviors, emotions_list, behaviors_list = generate_behaviors_for_emotions(\n",
    "    gred_model=gred_model_name.gred_model,\n",
    "    gred_tokenizer=gred_tokenizer,\n",
    "    emotions=emotions,\n",
    "    num_samples=20,  # Behaviors per run\n",
    "    num_runs=5  # Total runs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0376feea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Emotion\": emotions_list, \"Behavior\": behaviors_list})\n",
    "df.to_csv(\"gred_emro_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "46beafdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame({\"Emotion\": emotions_list, \"Behavior\": behaviors_list})\n",
    "#df.to_csv(\"gred_emro_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "97f070af",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = EMRODataset(behaviors_list, [emotion_to_label[e] for e in emotions_list], emro_tokenizer, max_len)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "64523cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating EMRO: 100%|██████████| 75/75 [00:02<00:00, 26.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMRO Test Loss: 0.1963, Accuracy: 0.9450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# After training is complete\n",
    "test_loss, test_accuracy = evaluate_emro_model(emro_model, test_loader, device)\n",
    "print(f\"EMRO Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14da951a",
   "metadata": {},
   "source": [
    "**Getting novelty scores for the behvaiors with EMRO loss**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7202e18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novelty Scores for each emotions\n",
      "disgust_surprise_alarm_fear:: 0.2546847462654114\n",
      "anger_frustration:: 0.05868440866470337\n",
      "joy_hope:: 0.20733439922332764\n",
      "confusion_sorrow_boredom:: 0.35945457220077515\n",
      "understanding_gratitude_relief:: 0.1733136773109436\n",
      "interest_desire:: 0.34272223711013794\n"
     ]
    }
   ],
   "source": [
    "embeddings_extractor = EmbeddingsExtractor(emro_model, emro_tokenizer, device)\n",
    "avg_novelty_by_emotion = evaluate_novelty(all_generated_behaviors, embeddings_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aec6b07",
   "metadata": {},
   "source": [
    "**Novelty loss backpropgataion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c728c359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50259, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50259, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gred_model.load_state_dict(torch.load('best_gred_model.pt', weights_only=True))\n",
    "gred_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c206d3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del gred_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d0f06c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class GREDwithNovelty(nn.Module):\n",
    "    def __init__(self, gred_model, gred_tokenizer, emro_model, emro_tokenizer, emotion_to_label, max_len=128):\n",
    "        super(GREDwithNovelty, self).__init__()\n",
    "        self.gred_model = gred_model\n",
    "        self.gred_tokenizer = gred_tokenizer\n",
    "        self.gred_model.resize_token_embeddings(len(self.gred_tokenizer))\n",
    "        \n",
    "        # EMRO model setup\n",
    "        self.emro_model = emro_model\n",
    "        self.emro_tokenizer = emro_tokenizer\n",
    "        self.emro_model.eval()  # EMRO used only for evaluation\n",
    "        for param in self.emro_model.parameters():\n",
    "            param.requires_grad = False  # Freeze EMRO parameters\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.gred_model.config.hidden_size, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 768),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Loss functions\n",
    "        self.emro_loss_fn = nn.CrossEntropyLoss()\n",
    "        #self.gred_loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        \n",
    "        # Emotion to Label mapping\n",
    "        self.emotion_to_label = emotion_to_label\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels, emotion_label):\n",
    "        # Forward pass through GRED\n",
    "        gred_outputs = self.gred_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            output_hidden_states=True \n",
    "        )\n",
    "        gred_loss = gred_outputs.loss  # Language modeling loss\n",
    "        hidden_states = gred_outputs.hidden_states[-1]  # Last hidden layer: (batch_size, seq_length, hidden_size)\n",
    "        #return gred_loss, hidden_states\n",
    "        pooled_hidden_states = hidden_states.mean(dim=1)  # (batch_size, hidden_size)\n",
    "        projected_embeddings = self.projection(pooled_hidden_states)  # (batch_size, 768)\n",
    "        \n",
    "        # Compute novelty loss\n",
    "        novelty_loss = self.compute_novelty_loss(projected_embeddings, emotion_label)\n",
    "    \n",
    "        total_loss =  gred_loss +  novelty_loss\n",
    "        \n",
    "        return total_loss, gred_loss, novelty_loss\n",
    "\n",
    "    def compute_novelty_loss(self, projected_embeddings, emotion_labels):\n",
    "        # Normalize embeddings\n",
    "        projected_embeddings = F.normalize(projected_embeddings, p=2, dim=1)\n",
    "        \n",
    "        # list to store average cosine similarities per emotion\n",
    "        avg_cosine_similarities = []\n",
    "        unique_emotions = emotion_labels.unique()\n",
    "        \n",
    "        for emotion in unique_emotions:\n",
    "            # Get indices of samples with the current emotion\n",
    "            indices = (emotion_labels == emotion).nonzero(as_tuple=True)[0]\n",
    "            if len(indices) < 2:\n",
    "              \n",
    "                avg_cosine_similarities.append(torch.tensor(0.0).to(projected_embeddings.device))\n",
    "                continue\n",
    "            # Extract embeddings for the current emotion\n",
    "            embeddings = projected_embeddings[indices]  # (num_samples, 768)\n",
    "            # Compute cosine similarity matrix\n",
    "            cosine_matrix = torch.mm(embeddings, embeddings.t())  # (num_samples, num_samples)\n",
    "            mask = torch.eye(cosine_matrix.size(0), device=projected_embeddings.device).bool()\n",
    "            cosine_matrix = cosine_matrix.masked_fill(mask, 0.0)\n",
    "            # Compute average cosine similarity\n",
    "            sum_cosine = cosine_matrix.sum()\n",
    "            num_pairs = (len(indices) * (len(indices) - 1))\n",
    "            avg_cosine = sum_cosine / num_pairs\n",
    "            avg_cosine_similarities.append(avg_cosine)\n",
    "        \n",
    "        # Compute novelty scores: 1 - average cosine similarity\n",
    "        novelty_scores = [1.0 - avg_cosine for avg_cosine in avg_cosine_similarities]\n",
    "        \n",
    "        # Compute novelty loss: 1 / (novelty + 0.001)\n",
    "        novelty_losses = [1.0 / (novelty + 0.001) for novelty in novelty_scores]\n",
    "        \n",
    "        # Average novelty loss across emotions\n",
    "        if len(novelty_losses) == 0:\n",
    "            novelty_loss = torch.tensor(0.0).to(projected_embeddings.device)\n",
    "        else:\n",
    "            novelty_loss = torch.stack(novelty_losses).mean()\n",
    "        \n",
    "        return novelty_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f4a916fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gred_train_dataset = GREDDataset(y_train, X_train, gred_tokenizer, max_len, emotion_to_label)\n",
    "\n",
    "gred_train_loader = DataLoader(gred_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "gred_with_novelty = GREDwithNovelty(\n",
    "    gred_model= gred_model,\n",
    "    gred_tokenizer=gred_tokenizer,\n",
    "    emro_model=emro_model,\n",
    "    emro_tokenizer=emro_tokenizer,\n",
    "    emotion_to_label=emotion_to_label,\n",
    "  ).to(device)\n",
    "\n",
    "gred_optimizer = torch.optim.AdamW(\n",
    "    list(gred_with_novelty.gred_model.parameters()) + list(gred_with_novelty.projection.parameters()),\n",
    "    lr=2e-5\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a15e7e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gred_with_novelty_feedback(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_gred_loss = 0.0\n",
    "    total_novelty_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        # Move data to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        emotion_labels = batch['emotion_label'].to(device)  # Original emotion label for EMRO\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through GRED model with Novelty feedback\n",
    "        total_batch_loss, gred_loss, novelty_loss = model(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            labels=labels, \n",
    "            emotion_label=emotion_labels\n",
    "        )\n",
    "\n",
    "        # Backpropagation\n",
    "        total_batch_loss.backward()\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track losses\n",
    "        total_loss += total_batch_loss.item()\n",
    "        total_gred_loss += gred_loss.item()\n",
    "        total_novelty_loss += novelty_loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    average_gred_loss = total_gred_loss / len(dataloader)\n",
    "    average_novelty_loss = total_novelty_loss / len(dataloader)\n",
    "    return average_loss, average_gred_loss, average_novelty_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6011bd5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/18\n",
      "Training Loss: 37.3462 | GRED Loss: 1.2282 | Novelty Loss: 45.1474\n",
      "\n",
      "Epoch 2/18\n",
      "Training Loss: 13.2834 | GRED Loss: 1.4538 | Novelty Loss: 14.7870\n",
      "\n",
      "Epoch 3/18\n",
      "Training Loss: 7.6532 | GRED Loss: 0.8257 | Novelty Loss: 8.5344\n",
      "\n",
      "Epoch 4/18\n",
      "Training Loss: 4.7484 | GRED Loss: 0.5754 | Novelty Loss: 5.2163\n",
      "\n",
      "Epoch 5/18\n",
      "Training Loss: 3.8303 | GRED Loss: 0.4245 | Novelty Loss: 4.2572\n",
      "\n",
      "Epoch 6/18\n",
      "Training Loss: 3.5111 | GRED Loss: 0.3832 | Novelty Loss: 3.9099\n",
      "\n",
      "Epoch 7/18\n",
      "Training Loss: 2.1538 | GRED Loss: 0.3895 | Novelty Loss: 2.2054\n",
      "\n",
      "Epoch 8/18\n",
      "Training Loss: 1.8368 | GRED Loss: 0.3858 | Novelty Loss: 1.8138\n",
      "\n",
      "Epoch 9/18\n",
      "Training Loss: 1.7227 | GRED Loss: 0.3801 | Novelty Loss: 1.6782\n",
      "\n",
      "Epoch 10/18\n",
      "Training Loss: 1.6650 | GRED Loss: 0.3697 | Novelty Loss: 1.6191\n",
      "\n",
      "Epoch 11/18\n",
      "Training Loss: 1.7685 | GRED Loss: 0.3549 | Novelty Loss: 1.7670\n",
      "\n",
      "Epoch 12/18\n",
      "Training Loss: 1.5943 | GRED Loss: 0.3489 | Novelty Loss: 1.5567\n",
      "\n",
      "Epoch 13/18\n",
      "Training Loss: 1.4671 | GRED Loss: 0.3450 | Novelty Loss: 1.4026\n",
      "\n",
      "Epoch 14/18\n",
      "Training Loss: 1.4460 | GRED Loss: 0.3378 | Novelty Loss: 1.3852\n",
      "\n",
      "Epoch 15/18\n",
      "Training Loss: 1.8396 | GRED Loss: 0.3368 | Novelty Loss: 1.8784\n",
      "\n",
      "Epoch 16/18\n",
      "Training Loss: 4.7068 | GRED Loss: 0.4750 | Novelty Loss: 5.2898\n",
      "\n",
      "Epoch 17/18\n",
      "Training Loss: 2.3225 | GRED Loss: 0.4678 | Novelty Loss: 2.3184\n",
      "\n",
      "Epoch 18/18\n",
      "Training Loss: 1.5939 | GRED Loss: 0.4602 | Novelty Loss: 1.4172\n"
     ]
    }
   ],
   "source": [
    "# Initialize Training Parameters\n",
    "best_val_loss = float('inf')\n",
    "patience = 2  # Number of epochs to wait before stopping\n",
    "counter = 0\n",
    "EPOCHS = 18\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    \n",
    "    # Training Phase\n",
    "    train_loss, train_gred_loss, train_novelty_loss = train_gred_with_novelty_feedback(\n",
    "        gred_with_novelty, gred_train_loader, gred_optimizer, device\n",
    "    )\n",
    "    print(f\"Training Loss: {train_loss:.4f} | GRED Loss: {train_gred_loss:.4f} | Novelty Loss: {train_novelty_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a67aca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(gred_with_novelty.state_dict(), 'final_gred_v3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3460fe9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GREDwithNovelty(\n",
       "  (gred_model): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50259, 1024)\n",
       "      (wpe): Embedding(1024, 1024)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-23): 24 x GPT2Block(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2SdpaAttention(\n",
       "            (c_attn): Conv1D(nf=3072, nx=1024)\n",
       "            (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "            (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1024, out_features=50259, bias=False)\n",
       "  )\n",
       "  (emro_model): RobertaClass(\n",
       "    (roberta): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): RobertaPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "    (loss_fn): CrossEntropyLoss()\n",
       "  )\n",
       "  (projection): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=768, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (emro_loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#del gred_with_novelty\n",
    "gred_with_novelty.load_state_dict(torch.load('final_gred_v3.pt', weights_only=True))\n",
    "gred_with_novelty.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9735a05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_generated_behaviors, emotions_list, behaviors_list = generate_behaviors_for_emotions(\n",
    "    gred_model=gred_with_novelty.gred_model,\n",
    "    gred_tokenizer=gred_tokenizer,\n",
    "    emotions=emotions,\n",
    "    num_samples=20,\n",
    "    num_runs = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b2cd8953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame({\"Emotion\": emotions_list, \"Behavior\": behaviors_list})\n",
    "#df.to_csv(\"final_gred_novelty_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aae6ddf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "#df = pd.read_csv(\"gred_novelty_data.csv\")\n",
    "\n",
    "# Convert into list of tuples (emotion, behavior)\n",
    "#all_generated_behaviors = [(row[\"Emotion\"], row[\"Behavior\"]) for _, row in df.iterrows()]\n",
    "# Extract behaviors and emotions as lists\n",
    "#behaviors_list = df[\"Behavior\"].tolist()\n",
    "#emotions_list = df[\"Emotion\"].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6b17f8",
   "metadata": {},
   "source": [
    "**Evaluating using Novelty Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f3378dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novelty Scores for each emotions\n",
      "confusion_sorrow_boredom:: 0.4086689352989197\n",
      "anger_frustration:: 0.21462202072143555\n",
      "interest_desire:: 0.27630847692489624\n",
      "joy_hope:: 0.3766695261001587\n",
      "disgust_surprise_alarm_fear:: 0.4894189238548279\n",
      "understanding_gratitude_relief:: 0.33106720447540283\n"
     ]
    }
   ],
   "source": [
    "embeddings_extractor = EmbeddingsExtractor(emro_model, emro_tokenizer, device)\n",
    "avg_novelty_by_emotion = evaluate_novelty(all_generated_behaviors, embeddings_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64b387b",
   "metadata": {},
   "source": [
    "**Evaluating Using EMRO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "adcfe676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating EMRO: 100%|██████████| 75/75 [00:02<00:00, 25.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMRO Test Loss: 1.4808, Accuracy: 0.5567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# After training is complete\n",
    "test_dataset = EMRODataset(behaviors_list, [emotion_to_label[e] for e in emotions_list], emro_tokenizer, max_len)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loss, test_accuracy = evaluate_emro_model(emro_model, test_loader, device)\n",
    "print(f\"EMRO Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8380e5a",
   "metadata": {},
   "source": [
    "**GRED with EMRO and Novelty**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7280352e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "#del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a00625c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50259, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50259, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gred_model.load_state_dict(torch.load('best_gred_model.pt', weights_only=True))\n",
    "gred_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b442c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GREDwithEMROandNovelty(nn.Module):\n",
    "    def __init__(self, gred_model, gred_tokenizer, \n",
    "                 emro_model, emro_tokenizer, emotion_to_label, \n",
    "                 alpha,  # weight for EMRO classification loss\n",
    "                 beta,   # weight for novelty loss\n",
    "                 max_len=128):\n",
    "        super().__init__()\n",
    "        self.gred_model = gred_model\n",
    "        self.gred_tokenizer = gred_tokenizer\n",
    "\n",
    "        #  freeze EMRO\n",
    "        self.emro_model = emro_model\n",
    "        self.emro_model.eval()\n",
    "        for param in self.emro_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.emro_tokenizer = emro_tokenizer\n",
    "        self.emotion_to_label = emotion_to_label\n",
    "\n",
    "        # output hidden states\n",
    "        self.gred_model.config.output_hidden_states = True\n",
    "\n",
    "        # Projection from GPT-2 hidden size to 768 (RoBERTa dimension)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.gred_model.config.hidden_size, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(768, 768),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Loss for EMRO classification\n",
    "        self.emro_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Weights for combining losses\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels, emotion_label):\n",
    "        gred_outputs = self.gred_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,            \n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        gred_loss = gred_outputs.loss  \n",
    "        hidden_states = gred_outputs.hidden_states[-1]\n",
    "\n",
    "        pooled_hidden_states = hidden_states.mean(dim=1)\n",
    "        projected_embeddings = self.projection(pooled_hidden_states)\n",
    "\n",
    "        # feed the projected embeddings into EMRO's classifier\n",
    "        emro_logits = self.emro_model.classifier(self.emro_model.dropout(projected_embeddings))\n",
    "        emro_loss = self.emro_loss_fn(emro_logits, emotion_label)\n",
    "        novelty_loss = self.compute_novelty_loss(projected_embeddings, emotion_label)\n",
    "        emro_loss = self.alpha * emro_loss\n",
    "        novelty_loss= self.beta * novelty_loss\n",
    "        # total_loss = GRED LM loss + alpha*EMRO + beta*Novelty\n",
    "        total_loss = gred_loss +  emro_loss + novelty_loss\n",
    "\n",
    "        return total_loss, gred_loss, emro_loss, novelty_loss\n",
    "\n",
    "    def compute_novelty_loss(self, projected_embeddings, emotion_labels):\n",
    "        normed_embs = F.normalize(projected_embeddings, p=2, dim=1)\n",
    "\n",
    "        unique_emotions = emotion_labels.unique()\n",
    "        avg_cosine_sims = []\n",
    "\n",
    "        for emo in unique_emotions:\n",
    "            indices = (emotion_labels == emo).nonzero(as_tuple=True)[0]\n",
    "            if len(indices) < 2:\n",
    "                avg_cosine_sims.append(torch.tensor(0.0, device=projected_embeddings.device))\n",
    "                continue\n",
    "            embs = normed_embs[indices]  # (num_samples, 768)\n",
    "\n",
    "            # Cosine matrix\n",
    "            cos_mat = torch.mm(embs, embs.t()) \n",
    "\n",
    "            # Zero out diagonal\n",
    "            diag_mask = torch.eye(cos_mat.size(0), device=cos_mat.device).bool()\n",
    "            cos_mat = cos_mat.masked_fill_(diag_mask, 0.0)\n",
    "\n",
    "            # sum of off-diagonal\n",
    "            sum_cos = cos_mat.sum()\n",
    "            num_pairs = len(indices) * (len(indices) - 1)\n",
    "            avg_cos = sum_cos / num_pairs  # average similarity\n",
    "\n",
    "            avg_cosine_sims.append(avg_cos)\n",
    "\n",
    "        # novelty = 1 - average cosine\n",
    "        # novelty_loss = average( 1 / (novelty+0.001) ) across emotions\n",
    "        novelty_scores = [1.0 - cs for cs in avg_cosine_sims]\n",
    "        novelty_vals = [1.0 / (ns + 0.001) for ns in novelty_scores]\n",
    "\n",
    "        if len(novelty_vals) == 0:\n",
    "            return torch.tensor(0.0, device=projected_embeddings.device)\n",
    "\n",
    "        novelty_loss = torch.stack(novelty_vals).mean()\n",
    "        return novelty_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "25115b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gred_with_emro_novelty_feedback(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss_all = 0.0\n",
    "    total_gred_loss = 0.0\n",
    "    total_emro_loss = 0.0\n",
    "    total_novelty_loss = 0.0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        emotion_labels = batch['emotion_label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # The unified forward returns 4 losses\n",
    "        total_batch_loss, gred_loss, emro_loss, novelty_loss = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            emotion_label=emotion_labels\n",
    "        )\n",
    "\n",
    "        total_batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss_all += total_batch_loss.item()\n",
    "        total_gred_loss += gred_loss.item()\n",
    "        total_emro_loss += emro_loss.item()\n",
    "        total_novelty_loss += novelty_loss.item()\n",
    "\n",
    "    n = len(dataloader)\n",
    "    return (\n",
    "        total_loss_all / n,\n",
    "        total_gred_loss / n,\n",
    "        total_emro_loss / n,\n",
    "        total_novelty_loss / n\n",
    "    )\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "15ddf472",
   "metadata": {},
   "outputs": [],
   "source": [
    "gred_emro_novelty_model = GREDwithEMROandNovelty(\n",
    "    gred_model=gred_model,\n",
    "    gred_tokenizer=gred_tokenizer,\n",
    "    emro_model=emro_model,\n",
    "    emro_tokenizer=emro_tokenizer,\n",
    "    emotion_to_label=emotion_to_label,\n",
    "    alpha= 0.915,  #0.95 \n",
    "    beta=0.085  #0.5\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(gred_emro_novelty_model.gred_model.parameters()) + list(gred_emro_novelty_model.projection.parameters()),\n",
    "    lr=2e-5\n",
    ")\n",
    "EPOCHS= 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "88759cc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/18\n",
      "Train -> total: 2.1298 | GRED: 0.2884 | EMRO: 1.6617 | Novelty: 0.1797\n",
      "\n",
      "Epoch 2/18\n",
      "Train -> total: 1.5728 | GRED: 0.2955 | EMRO: 1.0860 | Novelty: 0.1913\n",
      "\n",
      "Epoch 3/18\n",
      "Train -> total: 0.8274 | GRED: 0.3150 | EMRO: 0.3063 | Novelty: 0.2061\n",
      "\n",
      "Epoch 4/18\n",
      "Train -> total: 0.5367 | GRED: 0.2858 | EMRO: 0.0493 | Novelty: 0.2016\n",
      "\n",
      "Epoch 5/18\n",
      "Train -> total: 0.4930 | GRED: 0.2707 | EMRO: 0.0265 | Novelty: 0.1958\n",
      "\n",
      "Epoch 6/18\n",
      "Train -> total: 0.4615 | GRED: 0.2592 | EMRO: 0.0178 | Novelty: 0.1845\n",
      "\n",
      "Epoch 7/18\n",
      "Train -> total: 0.4574 | GRED: 0.2549 | EMRO: 0.0141 | Novelty: 0.1884\n",
      "\n",
      "Epoch 8/18\n",
      "Train -> total: 0.4261 | GRED: 0.2463 | EMRO: 0.0120 | Novelty: 0.1678\n",
      "\n",
      "Epoch 9/18\n",
      "Train -> total: 0.4207 | GRED: 0.2419 | EMRO: 0.0108 | Novelty: 0.1680\n",
      "\n",
      "Epoch 10/18\n",
      "Train -> total: 0.4018 | GRED: 0.2347 | EMRO: 0.0101 | Novelty: 0.1570\n",
      "\n",
      "Epoch 11/18\n",
      "Train -> total: 0.3192 | GRED: 0.2318 | EMRO: 0.0080 | Novelty: 0.0795\n",
      "\n",
      "Epoch 12/18\n",
      "Train -> total: 0.3133 | GRED: 0.2237 | EMRO: 0.0063 | Novelty: 0.0832\n",
      "\n",
      "Epoch 13/18\n",
      "Train -> total: 0.3070 | GRED: 0.2207 | EMRO: 0.0058 | Novelty: 0.0806\n",
      "\n",
      "Epoch 14/18\n",
      "Train -> total: 0.3026 | GRED: 0.2174 | EMRO: 0.0052 | Novelty: 0.0800\n",
      "\n",
      "Epoch 15/18\n",
      "Train -> total: 0.2979 | GRED: 0.2142 | EMRO: 0.0052 | Novelty: 0.0786\n",
      "\n",
      "Epoch 16/18\n",
      "Train -> total: 0.2968 | GRED: 0.2154 | EMRO: 0.0058 | Novelty: 0.0756\n",
      "\n",
      "Epoch 17/18\n",
      "Train -> total: 0.2835 | GRED: 0.1997 | EMRO: 0.0050 | Novelty: 0.0787\n",
      "\n",
      "Epoch 18/18\n",
      "Train -> total: 0.2768 | GRED: 0.1987 | EMRO: 0.0041 | Novelty: 0.0739\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    if epoch >= 10:\n",
    "        gred_emro_novelty_model.beta = 0.045\n",
    "        gred_emro_novelty_model.alpha = 0.955\n",
    "        \n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    train_loss, train_gred_loss, train_emro_loss, train_novelty_loss = train_gred_with_emro_novelty_feedback(\n",
    "        gred_emro_novelty_model, gred_train_loader, optimizer, device\n",
    "    )\n",
    "    print(f\"Train -> total: {train_loss:.4f} | GRED: {train_gred_loss:.4f} | EMRO: {train_emro_loss:.4f} | Novelty: {train_novelty_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05ebd302",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#torch.save(gred_emro_novelty_model.state_dict(), 'final_thesis_model.pt')\n",
    "#gred_emro_novelty_model.load_state_dict(torch.load('final_thesis_model.pt', weights_only=True)) #alpha = o.915 and 0.955 \n",
    "#gred_emro_novelty_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "25a84254",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_generated_behaviors, emotions_list, behaviors_list = generate_behaviors_for_emotions(\n",
    "    gred_model=gred_emro_novelty_model.gred_model,\n",
    "    gred_tokenizer=gred_tokenizer,\n",
    "    emotions=emotions,\n",
    "    num_samples=20,\n",
    "    num_runs = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d79535ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame({\"Emotion\": emotions_list, \"Behavior\": behaviors_list})\n",
    "#df.to_csv(\"final_thesis_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8d893906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "#df = pd.read_csv(\"Final_gred_version_data.csv\")\n",
    "\n",
    "# Convert into list of tuples (emotion, behavior)\n",
    "#all_generated_behaviors = [(row[\"Emotion\"], row[\"Behavior\"]) for _, row in df.iterrows()]\n",
    "# Extract behaviors and emotions as lists\n",
    "#behaviors_list = df[\"Behavior\"].tolist()\n",
    "#emotions_list = df[\"Emotion\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8f39a3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novelty Scores for each emotions\n",
      "anger_frustration:: 0.11802101135253906\n",
      "joy_hope:: 0.17870253324508667\n",
      "confusion_sorrow_boredom:: 0.358379602432251\n",
      "interest_desire:: 0.3516414165496826\n",
      "understanding_gratitude_relief:: 0.2546226978302002\n",
      "disgust_surprise_alarm_fear:: 0.28358232975006104\n"
     ]
    }
   ],
   "source": [
    "embeddings_extractor = EmbeddingsExtractor(emro_model, emro_tokenizer, device)\n",
    "avg_novelty_by_emotion = evaluate_novelty(all_generated_behaviors, embeddings_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc5b99e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating EMRO: 100%|██████████| 75/75 [00:02<00:00, 26.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMRO Test Loss: 0.2574, Accuracy: 0.9233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# After training is complete\n",
    "test_dataset = EMRODataset(behaviors_list, [emotion_to_label[e] for e in emotions_list], emro_tokenizer, max_len)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loss, test_accuracy = evaluate_emro_model(emro_model, test_loader, device)\n",
    "print(f\"EMRO Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
